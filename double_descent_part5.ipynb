{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: The Generalized Bias-Variance Tradeoff & Double Descent\n",
    "**A Non-Linear Programming Capstone Project**\n",
    "\n",
    "## 1. Notebook Objective & Theoretical Framework\n",
    "This final notebook synthesizes the analytical methods (Part 2), regularization theory (Part 3), and optimization behavior (Part 4) to demonstrate the **Double Descent** phenomenon.\n",
    "\n",
    "We will move beyond the \"single dataset\" view and adopt the statistical learning perspective (ISL Ch. 2 and Ch. 10.8). By simulating thousands of parallel universes (datasets), we will empirically decompose the Mean Squared Error into **Bias² + Variance + Irreducible Error**.\n",
    "\n",
    "**Core Hypothesis (based on *Schaeffer et al.* & *ISL*):**\n",
    "The \"descent\" in the over-parameterized regime ($p > n$) occurs because, among the infinite solutions that satisfy $X\\beta = y$, the \"natural\" solver (the Moore-Penrose Pseudoinverse or Gradient Descent initialized at zero) selects the solution with the **minimum $\\ell_2$ norm**. This acts as an *implicit* regularization, suppressing the variance that explodes at the interpolation threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block 1: The Experimental Design (The Ensemble Generator)\n",
    "**Goal:** Define the infrastructure to calculate \"True\" Bias and Variance.\n",
    "\n",
    "* **Concept:** To measure bias and variance, we cannot use a single training set. We must approximate the expectation over the data distribution $\\mathbb{E}_{\\mathcal{D}}$.\n",
    "* **Implementation Details:**\n",
    "    * Define a `true_function(x)`: $f(x) = \\sin(2\\pi x)$ or the previous $0.5x^2$.\n",
    "    * Create a factory function that generates $K$ distinct datasets (e.g., $K=100$), each with $N$ sample points (e.g., $N=15$).\n",
    "    * **PyTorch/Einsum:** Use broadcasting to generate all $K$ datasets in a single tensor operation for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "K_datasets = 1000   # Number of parallel universes (datasets)\n",
    "N_samples = 15      # Number of training points per dataset (Interpolation threshold at d=14)\n",
    "sigma_noise = 0.2   # Irreducible error (noise level)\n",
    "test_size = 1000    # Size of the test set for integral approximation\n",
    "\n",
    "# 1. Define the True Function\n",
    "def true_function(x):\n",
    "    # Using sin(2*pi*x) as it's a classic choice for showing wiggles, \n",
    "    # but we can also use 0.5*x**2 if preferred for continuity with Part 1.\n",
    "    # Let's use a slightly more complex function to justify high degrees.\n",
    "    return torch.sin(2 * torch.pi * x)\n",
    "\n",
    "# 2. The Ensemble Generator (Factory)\n",
    "def generate_ensemble(K, N, sigma):\n",
    "    \"\"\"\n",
    "    Generates K datasets, each with N points.\n",
    "    Returns:\n",
    "        X: (K, N) tensor of inputs\n",
    "        y: (K, N) tensor of targets\n",
    "    \"\"\"\n",
    "    # Random x in [-1, 1]\n",
    "    X = torch.rand(K, N) * 2 - 1\n",
    "    \n",
    "    # y = f(x) + epsilon\n",
    "    noise = torch.randn(K, N) * sigma\n",
    "    y = true_function(X) + noise\n",
    "    return X, y\n",
    "\n",
    "# Generate the Test Set (Fixed for all models)\n",
    "X_test = torch.linspace(-1, 1, test_size).view(-1, 1) # (T, 1)\n",
    "y_test_true = true_function(X_test)                   # (T, 1)\n",
    "\n",
    "print(f\"Generated {K_datasets} datasets with {N_samples} samples each.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block 2: The Solver & The Minimum Norm Solution\n",
    "**Goal:** Define the fitting mechanism that operates across both regimes.\n",
    "\n",
    "* **Concept:** We need a solver that works for $p < n$ (Classical) and $p > n$ (Over-parameterized).\n",
    "* **Mathematical Rigor:**\n",
    "    * For $p \\le n$ (Under-parameterized): The solution is unique (if $X$ is full rank). $\\hat{\\beta} = (X^T X)^{-1} X^T y$.\n",
    "    * For $p > n$ (Over-parameterized): The system is underdetermined. There are infinite solutions. We explicitly choose the **Minimum Norm Solution**: $\\hat{\\beta} = X^T (X X^T)^{-1} y$ (using the pseudoinverse definition).\n",
    "* **Implementation:** A function taking degree $d$ and the dataset, constructing the Vandermonde matrix via `einsum`, and solving via `torch.linalg.pinv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ensemble(X_train_ensemble, y_train_ensemble, degree):\n",
    "    \"\"\"\n",
    "    Fits polynomial models of 'degree' to all K datasets simultaneously.\n",
    "    \n",
    "    Args:\n",
    "        X_train_ensemble: (K, N) tensor\n",
    "        y_train_ensemble: (K, N) tensor\n",
    "        degree: int\n",
    "        \n",
    "    Returns:\n",
    "        betas: (K, d+1) tensor of coefficients\n",
    "    \"\"\"\n",
    "    K, N = X_train_ensemble.shape\n",
    "    \n",
    "    # 1. Construct Vandermonde Matrix for all K datasets\n",
    "    # We want a tensor of shape (K, N, d+1)\n",
    "    # Powers: [0, 1, ..., d]\n",
    "    powers = torch.arange(degree + 1).float()\n",
    "    \n",
    "    # Broadcasting magic:\n",
    "    # X_train_ensemble.unsqueeze(-1) is (K, N, 1)\n",
    "    # powers is (d+1)\n",
    "    # Result is (K, N, d+1)\n",
    "    Phi = X_train_ensemble.unsqueeze(-1) ** powers\n",
    "    \n",
    "    # 2. Solve for Beta using Pseudoinverse (Minimum Norm Solution)\n",
    "    # torch.linalg.pinv handles the batch dimension K automatically\n",
    "    # Phi: (K, N, d+1)\n",
    "    # y: (K, N)\n",
    "    \n",
    "    # We need y to be (K, N, 1) for matrix multiplication compatibility if we were doing it manually,\n",
    "    # but pinv expects standard shapes. Let's check pinv docs or usage.\n",
    "    # pinv(A) @ B is the typical solve.\n",
    "    \n",
    "    # Compute pseudoinverse of Phi: (K, d+1, N)\n",
    "    Phi_pinv = torch.linalg.pinv(Phi)\n",
    "    \n",
    "    # Beta = Phi_pinv @ y\n",
    "    # (K, d+1, N) @ (K, N, 1) -> (K, d+1, 1)\n",
    "    betas = torch.matmul(Phi_pinv, y_train_ensemble.unsqueeze(-1))\n",
    "    \n",
    "    return betas.squeeze(-1) # (K, d+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block 3: The Large-Scale Experiment (The Loop)\n",
    "**Goal:** Collect error metrics across the complexity spectrum.\n",
    "\n",
    "* **Methodology:**\n",
    "    * Iterate through model degrees $d$ from 1 to 50.\n",
    "    * For each degree $d$:\n",
    "        1.  Fit models to all $K$ datasets simultaneously.\n",
    "        2.  Evaluate predictions on a large, fixed **Test Set**.\n",
    "        3.  Calculate **Bias²**: $(\\mathbb{E}[\\hat{f}(x)] - f(x))^2$.\n",
    "        4.  Calculate **Variance**: $\\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2]$.\n",
    "        5.  Calculate **MSE**: Bias² + Variance + Noise.\n",
    "    * **Efficiency:** Heavily vectorized operations using Einstein summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for metrics\n",
    "degrees = range(1, 51)\n",
    "bias_squared_history = []\n",
    "variance_history = []\n",
    "mse_history = []\n",
    "avg_norm_history = []\n",
    "\n",
    "# Generate the datasets once\n",
    "X_train_K, y_train_K = generate_ensemble(K_datasets, N_samples, sigma_noise)\n",
    "\n",
    "print(\"Starting Double Descent Experiment...\")\n",
    "\n",
    "for d in degrees:\n",
    "    # 1. Fit models\n",
    "    betas = fit_ensemble(X_train_K, y_train_K, d) # (K, d+1)\n",
    "    \n",
    "    # 2. Store Norm of parameters (for Block 5)\n",
    "    # Average L2 norm across K models\n",
    "    avg_norm = torch.mean(torch.norm(betas, p=2, dim=1)).item()\n",
    "    avg_norm_history.append(avg_norm)\n",
    "    \n",
    "    # 3. Make Predictions on Test Set\n",
    "    # Construct Test Vandermonde: (T, d+1)\n",
    "    powers = torch.arange(d + 1).float()\n",
    "    Phi_test = X_test ** powers # (T, d+1)\n",
    "    \n",
    "    # Predictions: (K, T)\n",
    "    # We want y_pred[k, t] = Phi_test[t] @ betas[k]\n",
    "    # betas: (K, d+1), Phi_test: (T, d+1)\n",
    "    # Result: (K, T)\n",
    "    y_preds = torch.matmul(betas, Phi_test.T) \n",
    "    \n",
    "    # 4. Calculate Bias and Variance Decomposition\n",
    "    \n",
    "    # Expected Prediction E[f_hat(x)] over datasets: (T,)\n",
    "    main_prediction = torch.mean(y_preds, dim=0)\n",
    "    \n",
    "    # Bias^2: (E[f_hat] - f_true)^2\n",
    "    # Average over test points\n",
    "    bias_sq = torch.mean((main_prediction.unsqueeze(-1) - y_test_true) ** 2).item()\n",
    "    \n",
    "    # Variance: E[(f_hat - E[f_hat])^2]\n",
    "    # Variance per test point, then averaged\n",
    "    variance = torch.mean(torch.var(y_preds, dim=0)).item()\n",
    "    \n",
    "    # MSE: Average prediction error on test set\n",
    "    # Mean over K, Mean over T\n",
    "    # (y_preds - y_test_true.T)^2\n",
    "    mse = torch.mean((y_preds - y_test_true.T) ** 2).item()\n",
    "    \n",
    "    bias_squared_history.append(bias_sq)\n",
    "    variance_history.append(variance)\n",
    "    mse_history.append(mse)\n",
    "    \n",
    "    if d % 5 == 0:\n",
    "        print(f\"Degree {d}: MSE={mse:.4f}, Bias^2={bias_sq:.4f}, Var={variance:.4f}\")\n",
    "\n",
    "print(\"Experiment Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block 4: Visualization of the Double Descent\n",
    "**Goal:** The \"Money Plot\" (reproducing the YouTube video and ISL Figure 10.24).\n",
    "\n",
    "* **Visuals:** A single figure with three overlaid curves:\n",
    "    1.  **Bias² (Monotonic Decrease)**\n",
    "    2.  **Variance (The Bell Curve)**\n",
    "    3.  **Test Error (The Double Descent)**\n",
    "* **Annotation:** Explicitly mark the **Interpolation Threshold** ($p=n$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Plot Components\n",
    "plt.plot(degrees, bias_squared_history, label='Bias²', color='blue', linewidth=2, linestyle='--')\n",
    "plt.plot(degrees, variance_history, label='Variance', color='orange', linewidth=2, linestyle='--')\n",
    "plt.plot(degrees, mse_history, label='Test MSE (Risk)', color='red', linewidth=3)\n",
    "\n",
    "# Add Irreducible Error Line\n",
    "plt.axhline(y=sigma_noise**2, color='gray', linestyle=':', label='Irreducible Error (Noise)')\n",
    "\n",
    "# Mark Interpolation Threshold\n",
    "# Threshold is when number of parameters (d+1) equals number of samples (N)\n",
    "# d+1 = N => d = N-1\n",
    "threshold_degree = N_samples - 1\n",
    "plt.axvline(x=threshold_degree, color='black', linestyle='-', alpha=0.5)\n",
    "plt.text(threshold_degree + 0.5, max(mse_history)*0.8, 'Interpolation Threshold\\n(p = n)', fontsize=10)\n",
    "\n",
    "# Styling\n",
    "plt.yscale('log') # Log scale often makes the double descent clearer\n",
    "plt.xlabel('Polynomial Degree (Complexity)', fontsize=12)\n",
    "plt.ylabel('Error (Log Scale)', fontsize=12)\n",
    "plt.title('The Double Descent Phenomenon', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block 5: The Norm of the Parameters (Demystifying the Descent)\n",
    "**Goal:** Prove *why* the test error drops in the modern regime.\n",
    "\n",
    "* **Observation:** The norm will spike massively at $p=n$ (fighting to fit noise with limited freedom) and *decrease* as $p$ increases (the \"Minimum Norm\" effect).\n",
    "* **Conclusion:** In the over-parameterized regime, the extra dimensions allow the model to fit the training data perfectly while maintaining a *smaller* total vector norm than at the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(degrees, avg_norm_history, color='purple', linewidth=2)\n",
    "\n",
    "# Mark Threshold\n",
    "plt.axvline(x=threshold_degree, color='black', linestyle='-', alpha=0.5)\n",
    "plt.text(threshold_degree + 0.5, max(avg_norm_history)*0.8, 'Interpolation Threshold', fontsize=10)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Polynomial Degree', fontsize=12)\n",
    "plt.ylabel('Average L2 Norm of Beta (Log Scale)', fontsize=12)\n",
    "plt.title('Parameter Norm vs. Complexity', fontsize=16)\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
