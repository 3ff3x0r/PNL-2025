{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Regularization\n",
    "\n",
    "This section introduces a classic technique to combat the overfitting observed in Part 1 and the numerical instability from Part 2 by adding a penalty term to the loss function.\n",
    "\n",
    "* **Objective:** To control model complexity and prevent overfitting using **Ridge Regression (L2 Regularization)**, while understanding how regularization restores numerical stability.\n",
    "* **Methodology:**\n",
    "    1.  Modify the loss function to include a penalty term: $\\text{Loss} = \\text{MSE} + \\lambda ||\\beta||_2^2$.\n",
    "    2.  Implement the analytical solution for Ridge: $\\beta = (X^T X + \\lambda I)^{-1} X^T y$.\n",
    "    3.  Observe how the fitted model and its test error change as the regularization strength hyperparameter ($\\lambda$) is varied.\n",
    "    4.  Explore the transition toward the interpolation threshold to set up the double descent phenomenon (Part 5).\n",
    "* **Key Concepts:** Regularization, L2 Penalty (Ridge), Hyperparameter Tuning, Constrained Optimization, Bias-Variance Tradeoff, Numerical Stability, Interpolation Threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup: Reusing Functions from Previous Parts\n",
    "\n",
    "We'll reuse the data generation, matrix construction, and utility functions from Parts 1 and 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Data Setup Complete\n",
      "======================================================================\n",
      "Total samples: 200\n",
      "Training samples: 160\n",
      "Test samples: 40\n",
      "X range: [-3.0, 3.0]\n",
      "Noise standard deviation σ = 0.5\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data (matching Part 1 setup)\n",
    "n_samples = 200\n",
    "x_min, x_max = -3.0, 3.0\n",
    "sigma = 0.5\n",
    "\n",
    "# Generate x values uniformly\n",
    "x = torch.linspace(x_min, x_max, n_samples).unsqueeze(1)\n",
    "\n",
    "# Generate true function values\n",
    "y_true = 0.5 * x ** 2\n",
    "\n",
    "# Generate noise ε ~ N(0, σ²)\n",
    "epsilon = torch.normal(mean=0.0, std=sigma, size=(n_samples, 1))\n",
    "\n",
    "# Generate noisy observations\n",
    "y = y_true + epsilon\n",
    "\n",
    "# Train/Test Split (matching Part 1)\n",
    "train_ratio = 0.8\n",
    "n_train = int(n_samples * train_ratio)\n",
    "n_test = n_samples - n_train\n",
    "\n",
    "# Shuffle indices for random split\n",
    "indices = torch.randperm(n_samples)\n",
    "train_indices = indices[:n_train]\n",
    "test_indices = indices[n_train:]\n",
    "\n",
    "# Split the data\n",
    "x_train = x[train_indices]\n",
    "y_train = y[train_indices]\n",
    "x_test = x[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "# Function to construct Vandermonde matrix (from Part 2)\n",
    "def construct_vandermonde_matrix(x, degree):\n",
    "    \"\"\"\n",
    "    Construct Vandermonde matrix for power basis.\n",
    "    \n",
    "    For input x of shape (n, 1), returns matrix of shape (n, degree+1)\n",
    "    where each row is [1, x_i, x_i^2, ..., x_i^degree]\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    x_flat = x.squeeze()\n",
    "    \n",
    "    # Create matrix using broadcasting\n",
    "    powers = torch.arange(degree + 1, dtype=x.dtype, device=x.device)\n",
    "    vandermonde = x_flat.unsqueeze(1) ** powers.unsqueeze(0)\n",
    "    \n",
    "    return vandermonde\n",
    "\n",
    "# MSE function (from Part 1)\n",
    "def compute_mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error using Einstein summation.\n",
    "    MSE = mean((y_true - y_pred)^2)\n",
    "    \"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    mse = torch.einsum('ij,ij->', residuals, residuals) / residuals.numel()\n",
    "    return mse.item()\n",
    "\n",
    "# Prediction function (from Part 1)\n",
    "def predict_polynomial(x, coefficients):\n",
    "    \"\"\"\n",
    "    Predict using polynomial coefficients.\n",
    "    \"\"\"\n",
    "    degree = coefficients.shape[0] - 1\n",
    "    X_poly = construct_vandermonde_matrix(x, degree)\n",
    "    y_pred = torch.einsum('ij,jk->ik', X_poly, coefficients)\n",
    "    return y_pred\n",
    "\n",
    "# Pseudoinverse solver (from Part 2)\n",
    "def solve_pseudoinverse(X, y):\n",
    "    \"\"\"\n",
    "    Solve least squares using Moore-Penrose pseudoinverse: β = X^+ y.\n",
    "    This is numerically stable even for ill-conditioned matrices.\n",
    "    \"\"\"\n",
    "    X_pinv = torch.linalg.pinv(X)\n",
    "    y_flat = y.squeeze()\n",
    "    beta = torch.einsum('ij,j->i', X_pinv, y_flat).unsqueeze(1)\n",
    "    return beta\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Data Setup Complete\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total samples: {n_samples}\")\n",
    "print(f\"Training samples: {n_train}\")\n",
    "print(f\"Test samples: {n_test}\")\n",
    "print(f\"X range: [{x_min}, {x_max}]\")\n",
    "print(f\"Noise standard deviation σ = {sigma}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Regularization\n",
    "\n",
    "In Part 2, we identified two different issues that arise in high-degree polynomial regression:\n",
    "\n",
    "1. **Numerical instability** caused by the ill-conditioned Vandermonde matrix  \n",
    "2. **Statistical instability (overfitting)** caused by excessive model flexibility\n",
    "\n",
    "Part 2 successfully resolved (1):\n",
    "- By switching to orthogonal bases (e.g., Legendre polynomials), we obtained **well-conditioned** design matrices.\n",
    "- By using the pseudoinverse (SVD), we ensured **stable solutions** even when the matrix was nearly singular.\n",
    "\n",
    "However, solving the conditioning problem does **not** solve the **overfitting problem**.  \n",
    "Even with a perfectly conditioned basis, a degree-12 polynomial can still fit noise instead of signal.\n",
    "\n",
    "Part 3 now focuses on problem (2): controlling **model flexibility**.\n",
    "\n",
    "Regularization modifies the optimization problem itself, adding a penalty on large coefficients.  \n",
    "This stabilizes not the *numerics* (already fixed) but the *statistics* of the model—reducing variance and improving generalization.\n",
    "\n",
    "Ridge Regression (L2 regularization) provides a mathematically rigorous way to:\n",
    "- Constrain the size of the coefficients\n",
    "- Ensure a unique, stable minimizer\n",
    "- Balance bias and variance\n",
    "- Prevent overfitting even when conditioning is good\n",
    "\n",
    "#### Constrained Optimization and the Lagrangian Equivalence\n",
    "\n",
    "Now that the conditioning issue is resolved, we modify the optimization problem to control model *complexity* by introducing an explicit penalty on the coefficient norm.\n",
    "\n",
    "The unconstrained penalized objective:\n",
    "$$\n",
    "L(\\beta, \\lambda) = \\|X\\beta - y\\|_2^2 + \\lambda \\|\\beta\\|_2^2\n",
    "$$\n",
    "is equivalent to the constrained problem:\n",
    "$$\n",
    "\\min_{\\beta} \\|X\\beta - y\\|_2^2 \\quad \\text{subject to} \\quad \\|\\beta\\|_2^2 \\le c\n",
    "$$\n",
    "where $\\lambda$ is the Lagrangian multiplier.\n",
    "\n",
    "**Lagrangian Equivalence:**\n",
    "\n",
    "The penalized objective  \n",
    "$$ L(\\beta,\\lambda)=\\|X\\beta - y\\|^2 + \\lambda\\|\\beta\\|^2$$\n",
    "is the Lagrangian of the constrained problem  \n",
    "$$ \\min_{\\beta}\\|X\\beta - y\\|^2 \\quad \\text{s.t.} \\quad \\|\\beta\\|^2 \\le c,$$\n",
    "\n",
    "where $\\lambda \\ge 0$ is the Lagrange multiplier enforcing the constraint. Under convexity and Slater’s condition, the constrained and penalized formulations have *equivalent minimizers*.\n",
    "\n",
    "### Derivation of the Gradient (First-Order Optimality)\n",
    "\n",
    "Setting $\\nabla_{\\beta} L = 0$:\n",
    "$$\n",
    "2X^T(X\\beta - y) + 2\\lambda\\beta = 0\n",
    "$$\n",
    "The Hessian of the penalized objective is $H_{\\text{Ridge}} = 2(X^T X + \\lambda I)$. For $\\lambda > 0$, this matrix is *strictly positive definite*, ensuring:\n",
    "- *Strict convexity*\n",
    "- *Unique global minimizer*\n",
    "- *Numerical stability* (contrast with ill-conditioned $X^T X$ from Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Problem Setup:\n",
      "  Parameters (p): 13\n",
      "  Data Points (n): 160\n",
      "------------------------------------------------------------\n",
      "Cond(X) approx: 2349056.2638194794\n",
      "L2 Regularized Objective (Optimization Formulation):\n",
      "  Loss L(β) at random init: 1028084600580.3303\n",
      "  Gradient norm ||∇L(β)||:  2004572441710.5112\n",
      "  Gradient shape: torch.Size([13, 1]) (Expected: 13, 1)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Without Feature Scaling\n",
    "# ==============================================================================\n",
    "# We use the degree 12 polynomial from Part 2 to demonstrate stability\n",
    "DEGREE_ILL = 12\n",
    "# Re-construct matrices to ensure scope context\n",
    "X = construct_vandermonde_matrix(x_train, DEGREE_ILL) # Shape: (n_train, d)\n",
    "y_target = y_train                                    # Shape: (n_train, 1)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "print(f\"Optimization Problem Setup:\")\n",
    "print(f\"  Parameters (p): {n_features}\")\n",
    "print(f\"  Data Points (n): {X.shape[0]}\")\n",
    "#print(f\"Polynomial degrees: {DEGREE_ILL}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- 1. Objective Function ---\n",
    "def compute_regularized_loss_sse(X, y, beta, lambda_):\n",
    "    \"\"\"\n",
    "    Computes Regularized Loss using EINSUM.\n",
    "    L(β) = 0.5 * ||Xβ - y||^2 + 0.5 * λ * ||β||^2\n",
    "    \"\"\"\n",
    "    # 1. Prediction\n",
    "    # CORRECTION: We use 'i' for features in both tensors.\n",
    "    # 'ni' (X: n samples, i features)\n",
    "    # 'ik' (beta: i features, k output) -> 'nk' (pred: n samples, k output)\n",
    "    y_pred = torch.einsum('ni,ik->nk', X, beta) \n",
    "    \n",
    "    # 2. Residuals\n",
    "    residuals = y_pred - y \n",
    "    \n",
    "    # 3. SSE: Dot product of residuals with themselves\n",
    "    # 'nk,nk->' sums over all n and k, producing a scalar\n",
    "    sse = torch.einsum('nk,nk->', residuals, residuals)\n",
    "    \n",
    "    # 4. L2 Penalty: Dot product of beta with itself\n",
    "    beta_norm_sq = torch.einsum('ik,ik->', beta, beta)\n",
    "    \n",
    "    loss = 0.5 * sse + 0.5 * lambda_ * beta_norm_sq\n",
    "    return loss.item()\n",
    "\n",
    "def compute_exact_gradient(X, y, beta, lambda_):\n",
    "    \"\"\"\n",
    "    Computes exact gradient using EINSUM.\n",
    "    ∇L(β) = X^T(Xβ - y) + λβ\n",
    "    \"\"\"\n",
    "    # 1. Residual component\n",
    "    y_pred = torch.einsum('ni,ik->nk', X, beta)\n",
    "    residuals = y_pred - y \n",
    "    \n",
    "    # 2. Data Gradient: X^T * residuals\n",
    "    # Transpose is implicit in einsum by swapping indices\n",
    "    # X: 'ni', Residuals: 'nk'\n",
    "    # We want to sum over samples 'n', leaving 'i' and 'k' -> 'ik'\n",
    "    grad_data = torch.einsum('ni,nk->ik', X, residuals)\n",
    "    \n",
    "    # 3. Regularization Gradient\n",
    "    grad_reg = lambda_ * beta\n",
    "    \n",
    "    return grad_data + grad_reg\n",
    "  \n",
    "\n",
    "# ------------ Second implementation -----------------\n",
    "# def compute_regularized_loss_sse(X, y, beta, lambda_):\n",
    "#     \"\"\"\n",
    "#     L(β) = 0.5 * ||Xβ - y||^2 + 0.5 * λ * ||β||^2\n",
    "#     \"\"\"\n",
    "#     # y_pred: (n,1)\n",
    "#     y_pred = X @ beta\n",
    "#     residuals = y_pred - y\n",
    "\n",
    "#     # SSE as scalar\n",
    "#     sse = torch.sum(residuals * residuals)   # scalar\n",
    "\n",
    "#     # L2 penalty as scalar\n",
    "#     beta_norm_sq = torch.sum(beta * beta)\n",
    "\n",
    "#     loss = 0.5 * sse + 0.5 * lambda_ * beta_norm_sq\n",
    "#     return float(loss)    # ensure Python float for printing\n",
    "# -----------------------------------------------------------    \n",
    "\n",
    "    \n",
    "# ------------ Second implementation -----------------\n",
    "# def compute_exact_gradient(X, y, beta, lambda_):\n",
    "#     \"\"\"\n",
    "#     ∇L(β) = X^T(Xβ - y) + λβ\n",
    "#     \"\"\"\n",
    "#     y_pred = X @ beta\n",
    "#     residuals = y_pred - y\n",
    "\n",
    "#     grad_data = X.T @ residuals     # (d,1)\n",
    "#     grad_reg  = lambda_ * beta\n",
    "\n",
    "#     return grad_data + grad_reg\n",
    "# -----------------------------------------------------------    \n",
    "\n",
    "# --- 2. Verification ---\n",
    "\n",
    "# Initialize a random beta vector (representing a starting point x⁰ in optimization)\n",
    "beta_init = torch.randn(n_features, 1)\n",
    "lambda_val = 1.0\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# # Checking values to tell if the code is doing what it should do\n",
    "# # 1. shapes\n",
    "# print(\"X.shape:\", X.shape)\n",
    "# print(\"y.shape:\", y_target.shape)\n",
    "# print(\"beta_init.shape:\", beta_init.shape)\n",
    "\n",
    "# # 2. ranges\n",
    "# print(\"X min/max:\", X.min().item(), X.max().item())\n",
    "# print(\"y min/max:\", y.min().item(), y.max().item())\n",
    "# print(\"beta min/max:\", beta_init.min().item(), beta_init.max().item())\n",
    "\n",
    "# 3. SVD / condition number (useful to see ill-conditioning)\n",
    "sv = torch.linalg.svdvals(X)          # singular values sorted descending\n",
    "# print(\"singular values (first 5):\", sv[:5].cpu().numpy())\n",
    "cond_X = float(sv[0] / sv[-1]) if sv[-1] > 0 else float('inf')\n",
    "print(\"Cond(X) approx:\", cond_X)\n",
    "\n",
    "# # 4. quick residual check (with example beta)\n",
    "# y_pred = X @ beta_init\n",
    "# res = y_pred - y_target  # y_target is 160 samples\n",
    "# #res = y_pred - y - bug here, y is the full dataset\n",
    "# print(\"residuals min/max:\", res.min().item(), res.max().item(), \"res.norm:\", torch.norm(res).item())\n",
    "\n",
    "# # 5. compute one forward/backwards loss & grad with corrected funcs\n",
    "# loss = compute_regularized_loss_sse(X, y_target, beta_init, lambda_val)\n",
    "# grad = compute_exact_gradient(X, y_target, beta_init, lambda_val)\n",
    "# print(\"loss:\", loss, \"grad.norm:\", torch.norm(grad).item())\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "# Compute Loss and Gradient\n",
    "current_loss = compute_regularized_loss_sse(X, y_target, beta_init, lambda_val)\n",
    "current_grad = compute_exact_gradient(X, y_target, beta_init, lambda_val)\n",
    "\n",
    "print(\"L2 Regularized Objective (Optimization Formulation):\")\n",
    "print(f\"  Loss L(β) at random init: {current_loss:.4f}\")\n",
    "print(f\"  Gradient norm ||∇L(β)||:  {torch.norm(current_grad).item():.4f}\")\n",
    "\n",
    "# Theoretical Check: Dimensions\n",
    "print(f\"  Gradient shape: {current_grad.shape} (Expected: {n_features}, 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Numerical Preconditioning (Scaling) ---\n",
      "Scaled Feature Range: -4.47 to 4.64\n",
      "------------------------------------------------------------\n",
      "Einsum Loss:     960.8238\n",
      "Einsum Grad Norm:1123.8975\n",
      "\n",
      "--- Verification Results ---\n",
      "Cond(X) approx: 7595.4110895181575\n",
      "Loss Difference:     0.00000000  [OK]\n",
      "Gradient Difference: 0.00000000  [OK]\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 1: PREPROCESSING (Feature Scaling)\n",
    "# ==============================================================================\n",
    "print(\"--- 1. Numerical Preconditioning (Scaling) ---\")\n",
    "\n",
    "# 1. Construct the Raw Vandermonde Matrix\n",
    "DEGREE_ILL = 12\n",
    "X_train_raw = construct_vandermonde_matrix(x_train, DEGREE_ILL)\n",
    "X_test_raw  = construct_vandermonde_matrix(x_test, DEGREE_ILL)\n",
    "\n",
    "# 2. Implement Standardization (Z-Score Normalization)\n",
    "# Slice out the bias column (col 0) - NEVER scale the intercept!\n",
    "X_train_feats = X_train_raw[:, 1:] \n",
    "X_test_feats  = X_test_raw[:, 1:]\n",
    "\n",
    "# Calculate statistics (Train set only to avoid leakage)\n",
    "train_mean = X_train_feats.mean(dim=0, keepdim=True)\n",
    "train_std  = X_train_feats.std(dim=0, keepdim=True)\n",
    "\n",
    "# Apply Scaling (add epsilon 1e-8 for stability)\n",
    "X_train_scaled_feats = (X_train_feats - train_mean) / (train_std + 1e-8)\n",
    "X_test_scaled_feats  = (X_test_feats - train_mean) / (train_std + 1e-8)\n",
    "\n",
    "# 3. Reconstruct the full matrix (Bias + Scaled Features)\n",
    "bias_train = X_train_raw[:, :1]\n",
    "bias_test  = X_test_raw[:, :1]\n",
    "\n",
    "X_train = torch.cat([bias_train, X_train_scaled_feats], dim=1)\n",
    "X_test  = torch.cat([bias_test, X_test_scaled_feats], dim=1)\n",
    "\n",
    "# Update Global Variables\n",
    "X = X_train\n",
    "y_target = y_train\n",
    "n_features = X.shape[1]\n",
    "n_samples = X.shape[0]\n",
    "\n",
    "print(f\"Scaled Feature Range: {X.min().item():.2f} to {X.max().item():.2f}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 3: VERIFICATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Initialize random beta\n",
    "beta_init = torch.randn(n_features, 1)\n",
    "lambda_val = 1.0\n",
    "\n",
    "# 1. Run Your Method (Einsum)\n",
    "loss_einsum = compute_regularized_loss_sse(X, y_target, beta_init, lambda_val)\n",
    "grad_einsum = compute_exact_gradient(X, y_target, beta_init, lambda_val)\n",
    "\n",
    "print(f\"Einsum Loss:     {loss_einsum:.4f}\")\n",
    "print(f\"Einsum Grad Norm:{torch.norm(grad_einsum).item():.4f}\")\n",
    "\n",
    "# 2. Run Verification Method (PyTorch Autograd)\n",
    "# This uses a completely different engine (C++ backend) to check the math\n",
    "print(\"\\n--- Verification Results ---\")\n",
    "\n",
    "# Setup Autograd\n",
    "beta_auto = beta_init.clone().detach().requires_grad_(True)\n",
    "# We use standard @ operators here to ensure the \"second method\" is distinct\n",
    "y_pred_auto = X @ beta_auto\n",
    "loss_auto = 0.5 * torch.sum((y_pred_auto - y_target)**2) + 0.5 * lambda_val * torch.sum(beta_auto**2)\n",
    "loss_auto.backward()\n",
    "\n",
    "# Compare\n",
    "diff_loss = abs(loss_einsum - loss_auto.item())\n",
    "diff_grad = torch.norm(grad_einsum - beta_auto.grad).item()\n",
    "\n",
    "# 3. SVD / condition number (useful to see ill-conditioning)\n",
    "sv = torch.linalg.svdvals(X)          # singular values sorted descending\n",
    "# print(\"singular values (first 5):\", sv[:5].cpu().numpy())\n",
    "cond_X = float(sv[0] / sv[-1]) if sv[-1] > 0 else float('inf')\n",
    "print(\"Cond(X) approx:\", cond_X)\n",
    "\n",
    "print(f\"Loss Difference:     {diff_loss:.8f}  [{'OK' if diff_loss < 1e-5 else 'FAIL'}]\")\n",
    "print(f\"Gradient Difference: {diff_grad:.8f}  [{'OK' if diff_grad < 1e-5 else 'FAIL'}]\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verification on Project Data ---\n",
      "Dataset size: torch.Size([160, 13])\n",
      "Loss Difference:     0.00000000  [OK]\n",
      "Gradient Difference: 0.00000000  [OK]\n"
     ]
    }
   ],
   "source": [
    "# --- Using PyTorch Autograd ---\n",
    "\n",
    "# 1. Setup the \"Second Analysis\" (Autograd)\n",
    "# We need a copy of beta that PyTorch tracks for gradients\n",
    "beta_check = beta_init.clone().detach().requires_grad_(True)\n",
    "\n",
    "# 2. Calculate Loss using the \"Second Method\" (Standard PyTorch operations)\n",
    "# We recreate the math using simple operators to let PyTorch track it\n",
    "y_pred_check = X @ beta_check\n",
    "residuals_check = y_pred_check - y_target\n",
    "sse_check = torch.sum(residuals_check**2)\n",
    "reg_check = torch.sum(beta_check**2)\n",
    "loss_autograd = 0.5 * sse_check + 0.5 * lambda_val * reg_check\n",
    "\n",
    "# 3. Calculate Gradient using the \"Second Method\" (Backward Pass)\n",
    "loss_autograd.backward()\n",
    "grad_autograd = beta_check.grad\n",
    "\n",
    "# 4. Compare with Implementation \n",
    "# (Ensure you are using the Corrected/Second implementation functions)\n",
    "loss_manual = compute_regularized_loss_sse(X, y_target, beta_init, lambda_val)\n",
    "grad_manual = compute_exact_gradient(X, y_target, beta_init, lambda_val)\n",
    "\n",
    "print(f\"--- Verification on Project Data ---\")\n",
    "print(f\"Dataset size: {X.shape}\")\n",
    "\n",
    "# Compare Losses\n",
    "diff_loss = abs(loss_manual - loss_autograd.item())\n",
    "print(f\"Loss Difference:     {diff_loss:.8f}  [{'OK' if diff_loss < 1e-4 else 'FAIL'}]\")\n",
    "\n",
    "# Compare Gradients\n",
    "diff_grad = torch.norm(grad_manual - grad_autograd).item()\n",
    "print(f\"Gradient Difference: {diff_grad:.8f}  [{'OK' if diff_grad < 1e-4 else 'FAIL'}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE BLOCK 2: The Analytical Ridge Solution\n",
    "\n",
    "# --- 1. Implement Ridge Analytical Solver ---\n",
    "\n",
    "def solve_ridge_analytical(X, y, lambda_):\n",
    "    \"\"\"\n",
    "    Solves the Ridge Regression problem analytically: \n",
    "    β = (XᵀX + λI)⁻¹ Xᵀy.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : torch.Tensor, shape (n, d)\n",
    "        Design matrix\n",
    "    y : torch.Tensor, shape (n, 1)\n",
    "        Target vector\n",
    "    lambda_ : float\n",
    "        Regularization parameter\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    beta_ridge : torch.Tensor, shape (d, 1)\n",
    "        Ridge regression coefficients\n",
    "    cond_number : float\n",
    "        Condition number of the regularized matrix\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # 1. Compute XᵀX (Gram matrix): (n, d) @ (d, n) -> (d, d)\n",
    "    XTX = torch.einsum('ni,nj->ij', X, X)\n",
    "    \n",
    "    # 2. Compute Xᵀy (RHS vector): (d, n) @ (n, 1) -> (d, 1)\n",
    "    XTy = torch.einsum('ni,n->i', X, y.squeeze()).unsqueeze(1)\n",
    "    \n",
    "    # 3. Form the regularized matrix A = XᵀX + λI\n",
    "    # I: Identity matrix (d, d)\n",
    "    I = torch.eye(n_features, dtype=X.dtype, device=X.device)\n",
    "    A = XTX + lambda_ * I\n",
    "    \n",
    "    # 4. Solve the linear system Aβ = Xᵀy for β\n",
    "    # torch.linalg.solve is numerically superior to explicit inversion\n",
    "    beta_ridge = torch.linalg.solve(A, XTy)\n",
    "    \n",
    "    # Also compute the condition number to quantify stability\n",
    "    cond_number = torch.linalg.cond(A).item()\n",
    "    \n",
    "    return beta_ridge, cond_number\n",
    "\n",
    "# --- Demonstration of Numerical Stability ---\n",
    "# Use the ill-conditioned degree 12 polynomial from Part 2\n",
    "lambda_small = 1e-10  # Very small regularization (near OLS)\n",
    "lambda_large = 1e2    # Strong regularization\n",
    "\n",
    "# Solve for small lambda\n",
    "beta_small, cond_small = solve_ridge_analytical(X, y_train, lambda_small)\n",
    "\n",
    "# Solve for large lambda\n",
    "beta_large, cond_large = solve_ridge_analytical(X, y_train, lambda_large)\n",
    "\n",
    "# Compare with OLS condition number (from Part 2)\n",
    "XTX_ols = torch.einsum('ni,nj->ij', X, X)\n",
    "cond_ols = torch.linalg.cond(XTX_ols).item()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Conditioning: OLS (Ill-conditioned) vs. Ridge\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Matrix Condition Number of OLS (λ≈0): {cond_ols:.2e}\")\n",
    "print(f\"Matrix Condition Number of Ridge (λ={lambda_small:.1e}): {cond_small:.2e}\")\n",
    "print(f\"Matrix Condition Number of Ridge (λ={lambda_large:.0f}): {cond_large:.2e}\")\n",
    "print(f\"Reduction Factor (λ={lambda_large:.0f}): {cond_ols / cond_large:.2e}x\")\n",
    "print(f\"\\nOLS beta norm: ||β|| = {torch.norm(beta_small).item():.2f}\")\n",
    "print(f\"Ridge beta norm (λ={lambda_large:.0f}): ||β|| = {torch.norm(beta_large).item():.2f}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 3: Quantitative Analysis - Tuning $\\lambda$ and Exploring Complexity\n",
    "\n",
    "### Theory: The Bias-Variance Tradeoff in $\\lambda$\n",
    "\n",
    "| $\\lambda$ | Model Complexity | $\\|\\beta\\|_2$ | Bias | Variance | Effect |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---|\n",
    "| $\\lambda \\to 0$ | High | Large | Low | High | Overfitting (approaches OLS) |\n",
    "| $\\lambda \\to \\infty$ | Low | Small | High | Low | Underfitting (trivial solution) |\n",
    "| $\\lambda_{\\text{opt}}$ | Optimal | Medium | Balanced | Balanced | Generalization |\n",
    "\n",
    "### Code Block 3A: Lambda Sweep (Fixed Degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE BLOCK 3A: Sweeping the Hyperparameter λ\n",
    "\n",
    "# --- 1. Define Lambda Range (logarithmic sweep) ---\n",
    "# A broad range is needed to demonstrate the full effect\n",
    "lambda_min, lambda_max = -8, 4  # 10^-8 to 10^4\n",
    "lambda_values = torch.logspace(lambda_min, lambda_max, 50) \n",
    "\n",
    "# Use a power basis matrix for the overfitted problem (Degree 12)\n",
    "DEGREE_ILL = 12 \n",
    "X = construct_vandermonde_matrix(x_train, DEGREE_ILL)\n",
    "X_test = construct_vandermonde_matrix(x_test, DEGREE_ILL)\n",
    "\n",
    "# --- 2. Initialize storage for results ---\n",
    "train_mses = []\n",
    "test_mses = []\n",
    "beta_norms = []\n",
    "condition_numbers = []\n",
    "\n",
    "# --- 3. Iterate and Solve Analytically ---\n",
    "print(\"=\" * 60)\n",
    "print(f\"Analyzing Ridge Regression (Degree {DEGREE_ILL}) over 50 λ values\")\n",
    "print(\"-\" * 60)\n",
    "for lambda_val in lambda_values:\n",
    "    # A. Solve the analytical Ridge equation\n",
    "    beta_ridge, cond_num = solve_ridge_analytical(X, y_train, lambda_val.item())\n",
    "    \n",
    "    # B. Predict on train and test sets\n",
    "    y_train_pred = torch.einsum('ij,jk->ik', X, beta_ridge)\n",
    "    y_test_pred = torch.einsum('ij,jk->ik', X_test, beta_ridge)\n",
    "    \n",
    "    # C. Compute and store metrics\n",
    "    train_mses.append(compute_mse(y_train, y_train_pred))\n",
    "    test_mses.append(compute_mse(y_test, y_test_pred))\n",
    "    \n",
    "    # The L2 norm of the full vector is the penalty term\n",
    "    beta_norms.append(torch.norm(beta_ridge).item())\n",
    "    condition_numbers.append(cond_num)\n",
    "\n",
    "print(\"Analysis complete.\")\n",
    "print(f\"Min Test MSE: {min(test_mses):.4f} (Avg OLS Test MSE was: {test_mses[0]:.4f})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store results as tensors for plotting ease\n",
    "lambda_log = torch.log10(lambda_values)\n",
    "train_mses = torch.tensor(train_mses)\n",
    "test_mses = torch.tensor(test_mses)\n",
    "beta_norms = torch.tensor(beta_norms)\n",
    "condition_numbers = torch.tensor(condition_numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Block 3B: Degree Sweep (Critical for Double Descent Setup)\n",
    "\n",
    "**NEW**: This block is essential for setting up the double descent narrative. We sweep polynomial degree from 1 to ~30, approaching the interpolation threshold (where p ≈ n). For each degree, we compare:\n",
    "\n",
    "1. **Unregularized OLS** (using pseudoinverse for all degrees, as established in Part 2)\n",
    "2. **Optimal Ridge** (λ chosen to minimize test MSE)\n",
    "3. **Heavy Ridge** (large λ, high bias but stable)\n",
    "\n",
    "This reveals the classical U-shape (where regularization helps) and the transition point where conventional analysis breaks down, setting up Part 5's double descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE BLOCK 3B: Degree Sweep (Critical for Double Descent Setup)\n",
    "\n",
    "# --- Sweep polynomial degree from 1 to ~30 (approaching interpolation threshold) ---\n",
    "# The interpolation threshold occurs when p (parameters) ≈ n (data points)\n",
    "# For our data: n_train = 160, so threshold is around degree 159\n",
    "# We'll go up to degree 30 to show the transition region clearly\n",
    "\n",
    "degrees_range = list(range(1, 31))  # Degrees 1 to 30\n",
    "n_train_samples = x_train.shape[0]\n",
    "\n",
    "# Storage for results\n",
    "test_mse_ols = []\n",
    "test_mse_optimal_ridge = []\n",
    "test_mse_heavy_ridge = []\n",
    "beta_norms_ols = []\n",
    "beta_norms_optimal_ridge = []\n",
    "beta_norms_heavy_ridge = []\n",
    "condition_numbers_ols = []\n",
    "condition_numbers_ridge = []\n",
    "\n",
    "# For optimal Ridge, we'll do a quick lambda search for each degree\n",
    "# (using a smaller grid for efficiency)\n",
    "lambda_candidates = torch.logspace(-4, 2, 20)  # Reasonable range for most degrees\n",
    "heavy_lambda = 10.0  # Fixed large lambda for heavy regularization\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Degree Sweep: Comparing OLS, Optimal Ridge, and Heavy Ridge\")\n",
    "print(f\"Training samples: {n_train_samples} (interpolation threshold: degree ≈ {n_train_samples-1})\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for degree in degrees_range:\n",
    "    # Construct design matrices\n",
    "    X_train = construct_vandermonde_matrix(x_train, degree)\n",
    "    X_test = construct_vandermonde_matrix(x_test, degree)\n",
    "    n_params = X_train.shape[1]\n",
    "    \n",
    "    # --- 1. Unregularized OLS using pseudoinverse (as established in Part 2) ---\n",
    "    # This is mathematically honest and prevents runtime errors\n",
    "    # Near the interpolation threshold (p ≈ n), X^T X becomes singular/ill-conditioned\n",
    "    # Pseudoinverse handles these cases gracefully\n",
    "    beta_ols = solve_pseudoinverse(X_train, y_train)\n",
    "    y_test_pred_ols = torch.einsum('ij,jk->ik', X_test, beta_ols)\n",
    "    test_mse_ols.append(compute_mse(y_test, y_test_pred_ols))\n",
    "    beta_norms_ols.append(torch.norm(beta_ols).item())\n",
    "    \n",
    "    # Compute condition number for OLS (may be very large)\n",
    "    try:\n",
    "        XTX_ols = torch.einsum('ni,nj->ij', X_train, X_train)\n",
    "        cond_ols = torch.linalg.cond(XTX_ols).item()\n",
    "        condition_numbers_ols.append(cond_ols)\n",
    "    except:\n",
    "        condition_numbers_ols.append(float('inf'))\n",
    "    \n",
    "    # --- 2. Optimal Ridge (find λ that minimizes test MSE) ---\n",
    "    best_test_mse = float('inf')\n",
    "    best_lambda = None\n",
    "    best_beta = None\n",
    "    \n",
    "    for lambda_candidate in lambda_candidates:\n",
    "        beta_ridge, _ = solve_ridge_analytical(X_train, y_train, lambda_candidate.item())\n",
    "        y_test_pred_ridge = torch.einsum('ij,jk->ik', X_test, beta_ridge)\n",
    "        test_mse_ridge = compute_mse(y_test, y_test_pred_ridge)\n",
    "        \n",
    "        if test_mse_ridge < best_test_mse:\n",
    "            best_test_mse = test_mse_ridge\n",
    "            best_lambda = lambda_candidate.item()\n",
    "            best_beta = beta_ridge\n",
    "    \n",
    "    test_mse_optimal_ridge.append(best_test_mse)\n",
    "    beta_norms_optimal_ridge.append(torch.norm(best_beta).item())\n",
    "    \n",
    "    # --- 3. Heavy Ridge (fixed large λ) ---\n",
    "    beta_heavy, cond_ridge = solve_ridge_analytical(X_train, y_train, heavy_lambda)\n",
    "    y_test_pred_heavy = torch.einsum('ij,jk->ik', X_test, beta_heavy)\n",
    "    test_mse_heavy_ridge.append(compute_mse(y_test, y_test_pred_heavy))\n",
    "    beta_norms_heavy_ridge.append(torch.norm(beta_heavy).item())\n",
    "    condition_numbers_ridge.append(cond_ridge)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if degree % 5 == 0:\n",
    "        print(f\"Degree {degree:2d}: OLS MSE={test_mse_ols[-1]:.4f}, \"\n",
    "              f\"Opt Ridge (λ={best_lambda:.2e}) MSE={best_test_mse:.4f}, \"\n",
    "              f\"Heavy Ridge MSE={test_mse_heavy_ridge[-1]:.4f}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Degree sweep complete.\")\n",
    "print(f\"Interpolation threshold: p = n at degree ≈ {n_train_samples - 1}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Convert to tensors for plotting\n",
    "test_mse_ols = torch.tensor(test_mse_ols)\n",
    "test_mse_optimal_ridge = torch.tensor(test_mse_optimal_ridge)\n",
    "test_mse_heavy_ridge = torch.tensor(test_mse_heavy_ridge)\n",
    "degrees_tensor = torch.tensor(degrees_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 4: Visualization and Synthesis - The Complete Picture\n",
    "\n",
    "### Theory: Interpreting the Regularization Landscape\n",
    "\n",
    "This visualization synthesizes all concepts and sets up the double descent narrative:\n",
    "\n",
    "1. **Classical Regime (p < n)**: The U-shaped curve where regularization prevents overfitting\n",
    "2. **Transition (p ≈ n)**: The interpolation threshold where OLS becomes unstable\n",
    "3. **Overparameterized Regime (p > n)**: Preview of where double descent occurs (Part 5)\n",
    "\n",
    "### Code Block 4: Comprehensive Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE BLOCK 4: Comprehensive Visualization\n",
    "\n",
    "# --- Primary Visualization (Critical for narrative): Test MSE vs Degree ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Regularization Analysis: The Complete Picture', fontsize=16, weight='bold')\n",
    "\n",
    "# Plot 1: PRIMARY - Test MSE vs Polynomial Degree (Three Curves)\n",
    "axes[0].plot(degrees_tensor.numpy(), test_mse_ols.numpy(), 'r-o', \n",
    "             linewidth=2, markersize=5, label='Unregularized OLS', alpha=0.8)\n",
    "axes[0].plot(degrees_tensor.numpy(), test_mse_optimal_ridge.numpy(), 'b-s', \n",
    "             linewidth=2, markersize=5, label='Optimal Ridge', alpha=0.8)\n",
    "axes[0].plot(degrees_tensor.numpy(), test_mse_heavy_ridge.numpy(), 'k-^', \n",
    "             linewidth=2, markersize=5, label='Heavy Ridge (λ=10)', alpha=0.7)\n",
    "\n",
    "# Annotate interpolation threshold (p = n)\n",
    "interpolation_threshold = n_train_samples - 1\n",
    "axes[0].axvline(interpolation_threshold, color='gray', linestyle='--', \n",
    "                linewidth=1.5, alpha=0.5, label=f'Interpolation Threshold (p=n)')\n",
    "\n",
    "axes[0].set_xlabel('Polynomial Degree (Model Complexity)', fontsize=12)\n",
    "axes[0].set_ylabel('Test MSE', fontsize=12)\n",
    "axes[0].set_title('Test MSE vs. Model Complexity\\n(Classical U-Shape and Transition Region)', fontsize=13)\n",
    "axes[0].legend(fontsize=10, loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add text annotations\n",
    "axes[0].text(0.05, 0.95, 'Classical Regime\\n(p < n)\\nRegularization helps', \n",
    "             transform=axes[0].transAxes, fontsize=10, \n",
    "             verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', \n",
    "             facecolor='lightblue', alpha=0.5))\n",
    "axes[0].text(0.65, 0.95, 'Transition Region\\n(p ≈ n)\\nConventional analysis\\nbreaks down', \n",
    "             transform=axes[0].transAxes, fontsize=10, \n",
    "             verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', \n",
    "             facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Plot 2: Lambda Sweep Results (for degree 12)\n",
    "min_test_mse_idx = torch.argmin(test_mses)\n",
    "lambda_optimal = lambda_values[min_test_mse_idx]\n",
    "\n",
    "axes[1].plot(lambda_log.numpy(), train_mses.numpy(), 'o-', \n",
    "             linewidth=2, markersize=4, label='Training MSE', color='blue', alpha=0.7)\n",
    "axes[1].plot(lambda_log.numpy(), test_mses.numpy(), 's-', \n",
    "             linewidth=2, markersize=4, label='Test MSE', color='red', alpha=0.8)\n",
    "axes[1].axvline(lambda_log[min_test_mse_idx].item(), color='k', \n",
    "               linestyle='--', linewidth=1, label='Optimal $\\\\lambda$')\n",
    "\n",
    "axes[1].set_xlabel('$\\\\log_{10}(\\\\lambda)$', fontsize=12)\n",
    "axes[1].set_ylabel('Mean Squared Error', fontsize=12)\n",
    "axes[1].set_title(f'Generalization Error vs. Regularization Strength $\\\\lambda$\\n(Degree {DEGREE_ILL} Polynomial)', fontsize=13)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0, max(test_mses.max().item(), train_mses.max().item()) * 1.1)\n",
    "\n",
    "axes[1].text(-7, axes[1].get_ylim()[1] * 0.9, 'Overfitting\\n(High Variance)', \n",
    "             fontsize=10, horizontalalignment='left', \n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='salmon', alpha=0.3))\n",
    "axes[1].text(3, axes[1].get_ylim()[1] * 0.9, 'Underfitting\\n(High Bias)', \n",
    "             fontsize=10, horizontalalignment='right', \n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Secondary Visualizations ---\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Secondary Visualizations: Regularization Effects', fontsize=16, weight='bold')\n",
    "\n",
    "# Plot 1: Beta Norm Decay vs log(λ) (from lambda sweep)\n",
    "axes[0, 0].plot(lambda_log.numpy(), beta_norms.numpy(), 'k-', \n",
    "                linewidth=2, markersize=4, label='||$\\\\beta||_{2}$ Norm', alpha=0.8)\n",
    "axes[0, 0].axvline(lambda_log[min_test_mse_idx].item(), color='k', \n",
    "                   linestyle='--', linewidth=1)\n",
    "axes[0, 0].set_xlabel('$\\\\log_{10}(\\\\lambda)$', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Coefficient Norm $||\\\\beta||_{2}$', fontsize=12)\n",
    "axes[0, 0].set_title('Model Complexity $||\\\\beta||_{2}$ vs. Regularization Strength $\\\\lambda$', fontsize=13)\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_yscale('log')\n",
    "axes[0, 0].text(lambda_log[min_test_mse_idx].item(), \n",
    "                beta_norms[min_test_mse_idx].item() * 1.5, \n",
    "                f'Optimal $\\\\|\\\\beta\\\\|={beta_norms[min_test_mse_idx].item():.2f}$', \n",
    "                fontsize=10, bbox=dict(boxstyle='round,pad=0.5', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Plot 2: Condition Number Improvement vs log(λ)\n",
    "axes[0, 1].semilogy(lambda_log.numpy(), condition_numbers.numpy(), 'g-', \n",
    "                    linewidth=2, markersize=4, label='Condition Number', alpha=0.8)\n",
    "axes[0, 1].axvline(lambda_log[min_test_mse_idx].item(), color='k', \n",
    "                   linestyle='--', linewidth=1)\n",
    "axes[0, 1].set_xlabel('$\\\\log_{10}(\\\\lambda)$', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Condition Number $\\\\kappa(X^T X + \\\\lambda I)$', fontsize=12)\n",
    "axes[0, 1].set_title('Numerical Stability: Condition Number vs. $\\\\lambda$', fontsize=13)\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].text(-7, condition_numbers[0].item() * 0.5, \n",
    "                'Ill-conditioned\\n(near OLS)', fontsize=10, \n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='salmon', alpha=0.3))\n",
    "axes[0, 1].text(3, condition_numbers[-1].item() * 2, \n",
    "                'Well-conditioned\\n(strong regularization)', fontsize=10, \n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "# Plot 3: Model Fits Comparison (OLS, Optimal Ridge, Heavy Ridge) for degree 12\n",
    "# Solve for OLS (using pseudoinverse), Optimal Ridge, and Heavy Ridge\n",
    "X_plot = construct_vandermonde_matrix(x_train, DEGREE_ILL)\n",
    "beta_ols_plot = solve_pseudoinverse(X_plot, y_train)\n",
    "beta_optimal_plot, _ = solve_ridge_analytical(X_plot, y_train, lambda_optimal.item())\n",
    "beta_heavy_plot, _ = solve_ridge_analytical(X_plot, y_train, heavy_lambda)\n",
    "\n",
    "# Generate smooth x for plotting\n",
    "x_plot_smooth = torch.linspace(x_min, x_max, 300).unsqueeze(1)\n",
    "y_true_plot = 0.5 * x_plot_smooth ** 2\n",
    "\n",
    "y_ols_plot = predict_polynomial(x_plot_smooth, beta_ols_plot)\n",
    "y_optimal_plot = predict_polynomial(x_plot_smooth, beta_optimal_plot)\n",
    "y_heavy_plot = predict_polynomial(x_plot_smooth, beta_heavy_plot)\n",
    "\n",
    "axes[1, 0].scatter(x_train.numpy(), y_train.numpy(), alpha=0.6, \n",
    "                   label='Training data', s=40, color='gray')\n",
    "axes[1, 0].plot(x_plot_smooth.numpy(), y_true_plot.numpy(), 'g--', \n",
    "                linewidth=2, label='True function', alpha=0.7)\n",
    "axes[1, 0].plot(x_plot_smooth.numpy(), y_ols_plot.numpy(), 'r-', \n",
    "                linewidth=2, label=f'OLS (λ≈0) - Overfit', alpha=0.7)\n",
    "axes[1, 0].plot(x_plot_smooth.numpy(), y_optimal_plot.numpy(), 'b-', \n",
    "                linewidth=3, label=f'Ridge (Optimal $\\\\lambda$) - Balanced', alpha=0.9)\n",
    "axes[1, 0].plot(x_plot_smooth.numpy(), y_heavy_plot.numpy(), 'k-', \n",
    "                linewidth=2, label=f'Ridge (Heavy $\\\\lambda$) - Underfit', alpha=0.6)\n",
    "axes[1, 0].set_xlabel('x', fontsize=12)\n",
    "axes[1, 0].set_ylabel('y', fontsize=12)\n",
    "axes[1, 0].set_title(f'Visualizing the Bias-Variance Tradeoff with $\\\\lambda$ (Degree {DEGREE_ILL})', fontsize=13)\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Beta Norm vs Degree (showing complexity control)\n",
    "axes[1, 1].plot(degrees_tensor.numpy(), beta_norms_ols, 'r-o', \n",
    "                linewidth=2, markersize=4, label='OLS $||\\\\beta||_2$', alpha=0.8)\n",
    "axes[1, 1].plot(degrees_tensor.numpy(), beta_norms_optimal_ridge, 'b-s', \n",
    "                linewidth=2, markersize=4, label='Optimal Ridge $||\\\\beta||_2$', alpha=0.8)\n",
    "axes[1, 1].plot(degrees_tensor.numpy(), beta_norms_heavy_ridge, 'k-^', \n",
    "                linewidth=2, markersize=4, label='Heavy Ridge $||\\\\beta||_2$', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Polynomial Degree', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Coefficient Norm $||\\\\beta||_2$', fontsize=12)\n",
    "axes[1, 1].set_title('Model Complexity Control: $||\\\\beta||_2$ vs. Degree', fontsize=13)\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesis Points\n",
    "\n",
    "1. **Regularization as Numerical Stabilization**: The $\\lambda I$ term transforms an ill-conditioned problem (Part 2) into a well-conditioned, strictly convex optimization problem. The condition number reduction demonstrates how Ridge restores numerical stability that the Normal Equation $(X^T X)^{-1}$ lacked.\n",
    "\n",
    "2. **Regularization as Complexity Control**: The penalty term constrains model complexity, preventing overfitting in the classical regime (p < n). The U-shaped test error curve shows the bias-variance tradeoff in action, with optimal $\\lambda$ balancing fit and generalization.\n",
    "\n",
    "3. **The Interpolation Threshold**: As p approaches n, unregularized OLS becomes unstable (as seen in the exploding test MSE), but Ridge remains stable. This transition point is where the classical bias-variance analysis begins to break down, foreshadowing the double descent phenomenon (Part 5).\n",
    "\n",
    "4. **Connection to Part 2**: Regularization provides an analytical solution to the numerical instability that made the Normal Equation fail. The pseudoinverse (Part 2) was a workaround; Ridge is a principled solution that modifies the optimization problem itself.\n",
    "\n",
    "5. **Foreshadowing Part 4**: Iterative optimizers (Part 4) will solve the same regularized problem using gradient descent, Newton's method, and L-BFGS. The implicit bias of these algorithms relates to regularization's explicit bias, providing complementary perspectives on complexity control.\n",
    "\n",
    "6. **Foreshadowing Part 5**: The primary visualization (Test MSE vs Degree) shows the classical U-shape where regularization helps. Beyond the interpolation threshold (p > n), we will observe in Part 5 that test error can decrease again—the double descent phenomenon—challenging conventional wisdom about the bias-variance tradeoff.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
