{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Regularization\n",
    "\n",
    "This section introduces a classic technique to combat the overfitting observed in Part 1 and the numerical instability from Part 2 by adding a penalty term to the loss function.\n",
    "\n",
    "* **Objective:** To control model complexity and prevent overfitting using **Ridge Regression (L2 Regularization)**, while understanding how regularization restores numerical stability.\n",
    "* **Methodology:**\n",
    "    1.  Modify the loss function to include a penalty term: $\\text{Loss} = \\text{MSE} + \\lambda ||\\beta||_2^2$.\n",
    "    2.  Implement the analytical solution for Ridge: $\\beta = (X^T X + \\lambda I)^{-1} X^T y$.\n",
    "    3.  Observe how the fitted model and its test error change as the regularization strength hyperparameter ($\\lambda$) is varied.\n",
    "    4.  Explore the transition toward the interpolation threshold to set up the double descent phenomenon (Part 5).\n",
    "* **Key Concepts:** Regularization, L2 Penalty (Ridge), Hyperparameter Tuning, Constrained Optimization, Bias-Variance Tradeoff, Numerical Stability, Interpolation Threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup: Reusing Functions from Previous Parts\n",
    "\n",
    "We'll reuse the data generation, matrix construction, and utility functions from Parts 1 and 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data (matching Part 1 setup)\n",
    "n_samples = 200\n",
    "x_min, x_max = -3.0, 3.0\n",
    "sigma = 0.5\n",
    "\n",
    "# Generate x values uniformly\n",
    "x = torch.linspace(x_min, x_max, n_samples).unsqueeze(1)\n",
    "\n",
    "# Generate true function values\n",
    "y_true = 0.5 * x ** 2\n",
    "\n",
    "# Generate noise ε ~ N(0, σ²)\n",
    "epsilon = torch.normal(mean=0.0, std=sigma, size=(n_samples, 1))\n",
    "\n",
    "# Generate noisy observations\n",
    "y = y_true + epsilon\n",
    "\n",
    "# Train/Test Split (matching Part 1)\n",
    "train_ratio = 0.8\n",
    "n_train = int(n_samples * train_ratio)\n",
    "n_test = n_samples - n_train\n",
    "\n",
    "# Shuffle indices for random split\n",
    "indices = torch.randperm(n_samples)\n",
    "train_indices = indices[:n_train]\n",
    "test_indices = indices[n_train:]\n",
    "\n",
    "# Split the data\n",
    "x_train = x[train_indices]\n",
    "y_train = y[train_indices]\n",
    "x_test = x[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "# Function to construct Vandermonde matrix (from Part 2)\n",
    "def construct_vandermonde_matrix(x, degree):\n",
    "    \"\"\"\n",
    "    Construct Vandermonde matrix for power basis.\n",
    "    \n",
    "    For input x of shape (n, 1), returns matrix of shape (n, degree+1)\n",
    "    where each row is [1, x_i, x_i^2, ..., x_i^degree]\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    x_flat = x.squeeze()\n",
    "    \n",
    "    # Create matrix using broadcasting\n",
    "    powers = torch.arange(degree + 1, dtype=x.dtype, device=x.device)\n",
    "    vandermonde = x_flat.unsqueeze(1) ** powers.unsqueeze(0)\n",
    "    \n",
    "    return vandermonde\n",
    "\n",
    "# MSE function (from Part 1)\n",
    "def compute_mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error using Einstein summation.\n",
    "    MSE = mean((y_true - y_pred)^2)\n",
    "    \"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    mse = torch.einsum('ij,ij->', residuals, residuals) / residuals.numel()\n",
    "    return mse.item()\n",
    "\n",
    "# Prediction function (from Part 1)\n",
    "def predict_polynomial(x, coefficients):\n",
    "    \"\"\"\n",
    "    Predict using polynomial coefficients.\n",
    "    \"\"\"\n",
    "    degree = coefficients.shape[0] - 1\n",
    "    X_poly = construct_vandermonde_matrix(x, degree)\n",
    "    y_pred = torch.einsum('ij,jk->ik', X_poly, coefficients)\n",
    "    return y_pred\n",
    "\n",
    "# Pseudoinverse solver (from Part 2)\n",
    "def solve_pseudoinverse(X, y):\n",
    "    \"\"\"\n",
    "    Solve least squares using Moore-Penrose pseudoinverse: β = X^+ y.\n",
    "    This is numerically stable even for ill-conditioned matrices.\n",
    "    \"\"\"\n",
    "    X_pinv = torch.linalg.pinv(X)\n",
    "    y_flat = y.squeeze()\n",
    "    beta = torch.einsum('ij,j->i', X_pinv, y_flat).unsqueeze(1)\n",
    "    return beta\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Data Setup Complete\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total samples: {n_samples}\")\n",
    "print(f\"Training samples: {n_train}\")\n",
    "print(f\"Test samples: {n_test}\")\n",
    "print(f\"X range: [{x_min}, {x_max}]\")\n",
    "print(f\"Noise standard deviation σ = {sigma}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Regularization\n",
    "\n",
    "In Part 2, we identified two different issues that arise in high-degree polynomial regression:\n",
    "\n",
    "1. **Numerical instability** caused by the ill-conditioned Vandermonde matrix  \n",
    "2. **Statistical instability (overfitting)** caused by excessive model flexibility\n",
    "\n",
    "Part 2 successfully resolved (1):\n",
    "- By switching to orthogonal bases (e.g., Legendre polynomials), we obtained **well-conditioned** design matrices.\n",
    "- By using the pseudoinverse (SVD), we ensured **stable solutions** even when the matrix was nearly singular.\n",
    "\n",
    "However, solving the conditioning problem does **not** solve the **overfitting problem**.  \n",
    "Even with a perfectly conditioned basis, a degree-12 polynomial can still fit noise instead of signal.\n",
    "\n",
    "Part 3 now focuses on problem (2): controlling **model flexibility**.\n",
    "\n",
    "Regularization modifies the optimization problem itself, adding a penalty on large coefficients.  \n",
    "This stabilizes not the *numerics* (already fixed) but the *statistics* of the model—reducing variance and improving generalization.\n",
    "\n",
    "Ridge Regression (L2 regularization) provides a mathematically rigorous way to:\n",
    "- Constrain the size of the coefficients\n",
    "- Ensure a unique, stable minimizer\n",
    "- Balance bias and variance\n",
    "- Prevent overfitting even when conditioning is good\n",
    "\n",
    "#### Constrained Optimization and the Lagrangian Equivalence\n",
    "\n",
    "Now that the conditioning issue is resolved, we modify the optimization problem to control model *complexity* by introducing an explicit penalty on the coefficient norm.\n",
    "\n",
    "The unconstrained penalized objective:\n",
    "$$\n",
    "L(\\beta, \\lambda) = \\|X\\beta - y\\|_2^2 + \\lambda \\|\\beta\\|_2^2\n",
    "$$\n",
    "is equivalent to the constrained problem:\n",
    "$$\n",
    "\\min_{\\beta} \\|X\\beta - y\\|_2^2 \\quad \\text{subject to} \\quad \\|\\beta\\|_2^2 \\le c\n",
    "$$\n",
    "where $\\lambda$ is the Lagrangian multiplier.\n",
    "\n",
    "**Lagrangian Equivalence:**\n",
    "\n",
    "The penalized objective  \n",
    "$$ L(\\beta,\\lambda)=\\|X\\beta - y\\|^2 + \\lambda\\|\\beta\\|^2$$\n",
    "is the Lagrangian of the constrained problem  \n",
    "$$ \\min_{\\beta}\\|X\\beta - y\\|^2 \\quad \\text{s.t.} \\quad \\|\\beta\\|^2 \\le c,$$\n",
    "\n",
    "where $\\lambda \\ge 0$ is the Lagrange multiplier enforcing the constraint. Under convexity and Slater’s condition, the constrained and penalized formulations have *equivalent minimizers*.\n",
    "\n",
    "### Derivation of the Gradient (First-Order Optimality)\n",
    "\n",
    "Setting $\\nabla_{\\beta} L = 0$:\n",
    "$$\n",
    "2X^T(X\\beta - y) + 2\\lambda\\beta = 0\n",
    "$$\n",
    "The Hessian of the penalized objective is $H_{\\text{Ridge}} = 2(X^T X + \\lambda I)$. For $\\lambda > 0$, this matrix is *strictly positive definite*, ensuring:\n",
    "- *Strict convexity*\n",
    "- *Unique global minimizer*\n",
    "- *Numerical stability* (contrast with ill-conditioned $X^T X$ from Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 1: The Optimization Landscape (The \"Cliff\")\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# Force Double Precision (float64) to measure the \"explosion\" accurately without NaNs\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --- 2. SETUP (Raw, Unscaled Data) ---\n",
    "# We use Degree 12 to ensure the problem is ill-conditioned (The \"Villain\")\n",
    "DEGREE_ILL = 12\n",
    "\n",
    "# Re-construct matrices using the functions from your setup block\n",
    "# We cast to .double() to ensure precision matches the configuration\n",
    "X = construct_vandermonde_matrix(x_train, DEGREE_ILL).double()\n",
    "y_target = y_train.double()\n",
    "n_features = X.shape[1]\n",
    "\n",
    "print(f\"Optimization Setup:\")\n",
    "print(f\"  Polynomial Degree: {DEGREE_ILL}\")\n",
    "print(f\"  Parameters (p):    {n_features}\")\n",
    "print(f\"  Data Points (n):   {X.shape[0]}\")\n",
    "\n",
    "# --- 3. LOSS & GRADIENT (Autograd) ---\n",
    "# We use Autograd to prove this is a property of the data geometry.\n",
    "\n",
    "def compute_loss_autograd(X, y, beta, lambda_):\n",
    "    \"\"\" \n",
    "    Computes Regularized Loss: L(β) = 0.5*SSE + 0.5*λ*||β||^2 \n",
    "    \"\"\"\n",
    "    # 1. Prediction (Standard Matrix Multiplication is safe here)\n",
    "    y_pred = X @ beta \n",
    "    \n",
    "    # 2. Loss Components\n",
    "    # 0.5 scaling is standard in optimization to cancel the '2' in the derivative\n",
    "    sse = 0.5 * torch.sum((y_pred - y)**2)\n",
    "    l2_penalty = 0.5 * lambda_ * torch.sum(beta**2)\n",
    "    \n",
    "    return sse + l2_penalty\n",
    "\n",
    "# Initialize random weights (The \"Start Point\")\n",
    "# We set requires_grad=True so PyTorch tracks the slope\n",
    "beta_init = torch.randn(n_features, 1, requires_grad=True)\n",
    "lambda_val = 0.0 # No regularization yet (OLS equivalent)\n",
    "\n",
    "# Forward Pass\n",
    "loss = compute_loss_autograd(X, y_target, beta_init, lambda_val)\n",
    "\n",
    "# Backward Pass (Calculating the Gradient)\n",
    "loss.backward()\n",
    "grad = beta_init.grad\n",
    "\n",
    "# --- 4. DIAGNOSTICS ---\n",
    "# Measure the \"Squashed Valley\" geometry\n",
    "sv = torch.linalg.svdvals(X)\n",
    "cond_X = (sv[0] / sv[-1]).item()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Condition Number (X): {cond_X:.2e}\")\n",
    "print(f\"Initial Loss:         {loss.item():.2e}\")\n",
    "print(f\"Initial Gradient Norm:{torch.norm(grad).item():.2e}\")\n",
    "print(\"-\" * 60)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"1. The Condition Number (10^6) proves the feature columns are highly correlated.\")\n",
    "print(\"2. The Gradient Norm (10^12) confirms the landscape is a vertical cliff.\")\n",
    "print(\"   Any standard step size (learning rate) would cause the optimizer to overshoot\")\n",
    "print(\"   and explode. This necessitates the Ridge solution in the next block.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 2: Feature Scaling (Standardization)\n",
    "\n",
    "\n",
    "# --- 1. SCALING SETUP ---\n",
    "# We treat the \"Raw\" X from Block 1 as our starting point\n",
    "X_raw = X.numpy() # Convert back to numpy for sklearn\n",
    "\n",
    "# Initialize Scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# CRITICAL: Protect the Intercept\n",
    "# The first column is all 1s (Intercept). Variance is 0. Scaling it creates NaNs.\n",
    "# We split it off, scale the rest, and re-attach it.\n",
    "X_feat = X_raw[:, 1:] \n",
    "X_feat_scaled = scaler.fit_transform(X_feat)\n",
    "\n",
    "# FIX 1: Use the correct variable name 'X_feat_scaled'\n",
    "X_scaled = np.hstack([X_raw[:, :1], X_feat_scaled])\n",
    "\n",
    "# Convert back to Tensor (Double Precision)\n",
    "X_torch = torch.tensor(X_scaled, dtype=torch.float64)\n",
    "\n",
    "# --- 2. DIAGNOSTICS ON SCALED DATA ---\n",
    "# We run the exact same checks as Block 1 to compare.\n",
    "\n",
    "# Initialize random weights for the new, scaled feature space\n",
    "# Note: We need a fresh beta because the feature semantics have changed\n",
    "beta_scaled = torch.randn(n_features, 1, requires_grad=True)\n",
    "\n",
    "# Forward & Backward (Using the same Autograd function from Block 1)\n",
    "# FIX 2: Correct argument name 'lambda_' instead of 'lambda_val'\n",
    "loss_scaled = compute_loss_autograd(X_torch, y_target, beta_scaled, lambda_=0.0)\n",
    "loss_scaled.backward()\n",
    "grad_scaled = beta_scaled.grad\n",
    "\n",
    "# Condition Number\n",
    "sv_scaled = torch.linalg.svdvals(X_torch)\n",
    "cond_X_scaled = (sv_scaled[0] / sv_scaled[-1]).item()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Optimization Setup (Scaled):\")\n",
    "print(f\"  Condition Number (X): {cond_X_scaled:.2e}\")\n",
    "print(f\"  Initial Loss:         {loss_scaled.item():.2e}\")\n",
    "print(f\"  Initial Gradient Norm:{torch.norm(grad_scaled).item():.2e}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- 3. COMPARISON ---\n",
    "# (Assuming 'cond_X' and 'grad' from Block 1 are still in memory)\n",
    "print(\"IMPACT OF SCALING:\")\n",
    "print(f\"1. Condition Number Improvement: {cond_X / cond_X_scaled:.1e}x\")\n",
    "print(f\"   (From {cond_X:.1e} -> {cond_X_scaled:.1e})\")\n",
    "# We use .item() to get scalar values for division\n",
    "grad_norm_raw = torch.norm(grad).item()\n",
    "grad_norm_scaled = torch.norm(grad_scaled).item()\n",
    "print(f\"2. Gradient Norm Reduction:      {grad_norm_raw / grad_norm_scaled:.1e}x\")\n",
    "print(\"   (The vertical cliff has become a manageable hill.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory: The Bias-Variance Tradeoff in $\\lambda$\n",
    "\n",
    "| $\\lambda$ | Model Complexity | $\\|\\beta\\|_2$ | Bias | Variance | Effect |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---|\n",
    "| $\\lambda \\to 0$ | High | Large | Low | High | Overfitting (approaches OLS) |\n",
    "| $\\lambda \\to \\infty$ | Low | Small | High | Low | Underfitting (trivial solution) |\n",
    "| $\\lambda_{\\text{opt}}$ | Optimal | Medium | Balanced | Balanced | Generalization |\n",
    "\n",
    "### Lambda Sweep (Fixed Degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 3: Analytical Solution & Hyperparameter Tuning (The Fix)\n",
    "\n",
    "# --- 1. PREPARE TEST DATA (Crucial Step) ---\n",
    "# We must scale the test set using the SAME scaler fitted on training data.\n",
    "# If we fit a new scaler on test data, we break data leakage rules.\n",
    "\n",
    "X_test_raw = construct_vandermonde_matrix(x_test, DEGREE_ILL).numpy()\n",
    "y_test_target = y_test.double()\n",
    "\n",
    "# Split intercept, transform features using Block 2's 'scaler', re-attach intercept\n",
    "X_test_feat = X_test_raw[:, 1:]\n",
    "X_test_feat_scaled = scaler.transform(X_test_feat) # Note: .transform(), NOT .fit_transform()\n",
    "X_test_scaled = np.hstack([X_test_raw[:, :1], X_test_feat_scaled])\n",
    "\n",
    "# Convert to Tensor\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float64)\n",
    "\n",
    "# --- 2. THE ANALYTICAL SOLVER (Ridge) ---\n",
    "\n",
    "def solve_ridge_analytical(X, y, lambda_):\n",
    "    \"\"\"\n",
    "    Solves (X'X + λI)β = X'y using Cholesky decomposition.\n",
    "    This leverages the Positive Definiteness guaranteed by Regularization.\n",
    "    \"\"\"\n",
    "    # Ensure 2D shapes\n",
    "    if y.ndim == 1: y = y.view(-1, 1)\n",
    "    n, d = X.shape\n",
    "\n",
    "    # Construct Linear System: Aβ = b\n",
    "    # A = X^T X + λI\n",
    "    XTX = X.T @ X\n",
    "    XTy = X.T @ y\n",
    "    I = torch.eye(d, device=X.device, dtype=X.dtype)\n",
    "    A = XTX + (lambda_ * I)\n",
    "\n",
    "    # Solve using Cholesky (Faster/More stable than standard inversion)\n",
    "    # Because A is SPD for lambda > 0, Cholesky is the preferred method.\n",
    "    try:\n",
    "        L = torch.linalg.cholesky(A)\n",
    "        beta = torch.cholesky_solve(XTy, L)\n",
    "    except RuntimeError:\n",
    "        # Fallback to standard solve if numerical noise makes A indefinite near 0\n",
    "        beta = torch.linalg.solve(A, XTy)\n",
    "        \n",
    "    return beta\n",
    "\n",
    "# --- 3. THE SWEEP (Bias-Variance Tradeoff) ---\n",
    "\n",
    "# Define Lambda Range\n",
    "# Since data is scaled, we don't need massive lambdas. 10^-4 to 10^4 is sufficient.\n",
    "lambda_values = torch.logspace(-4, 4, 50)\n",
    "\n",
    "# Storage\n",
    "history = {'lambda': [], 'train_mse': [], 'test_mse': [], 'beta_norm': []}\n",
    "\n",
    "print(f\"Sweeping {len(lambda_values)} lambda values on Scaled Data...\")\n",
    "\n",
    "for lam in lambda_values:\n",
    "    lambda_val = lam.item()\n",
    "    \n",
    "    # A. Solve\n",
    "    beta = solve_ridge_analytical(X_torch, y_target, lambda_val)\n",
    "    \n",
    "    # B. Predict\n",
    "    y_train_pred = X_torch @ beta\n",
    "    y_test_pred = X_test_torch @ beta\n",
    "    \n",
    "    # C. Measure Errors\n",
    "    train_mse = torch.mean((y_train_pred - y_target.view(-1,1))**2).item()\n",
    "    test_mse = torch.mean((y_test_pred - y_test_target.view(-1,1))**2).item()\n",
    "    norm_beta = torch.norm(beta).item()\n",
    "    \n",
    "    # Store\n",
    "    history['lambda'].append(lambda_val)\n",
    "    history['train_mse'].append(train_mse)\n",
    "    history['test_mse'].append(test_mse)\n",
    "    history['beta_norm'].append(norm_beta)\n",
    "\n",
    "# --- 4. RESULTS ---\n",
    "# Find the lambda that minimized Test MSE\n",
    "best_idx = np.argmin(history['test_mse'])\n",
    "best_lambda = history['lambda'][best_idx]\n",
    "min_mse = history['test_mse'][best_idx]\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Optimal Lambda: {best_lambda:.4f}\")\n",
    "print(f\"Min Test MSE:   {min_mse:.4f}\")\n",
    "print(f\"Beta Norm at Opt: {history['beta_norm'][best_idx]:.2f}\")\n",
    "print(\"-\" * 60)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"1. We found the 'Sweet Spot' where the model generalizes best.\")\n",
    "print(\"2. If we plotted this, we would see the classical U-shaped curve.\")\n",
    "print(\"3. The Beta Norm is controlled, proving we solved the 'Statistical Instability'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Sweep \n",
    "\n",
    "We sweep polynomial degree from 1 to ~100, approaching the interpolation threshold (where p ≈ n). For each degree, we compare:\n",
    "\n",
    "1. **Unregularized OLS**: The model's \"best effort\" to fit the data, solved using the numerically stable Pseudoinverse.\n",
    "2. **Regularized Ridge**: Solved using a fixed, optimized λ.\n",
    "\n",
    "This reveals the classical U-shape where regularization helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 4: Comparative Stability (The \"Stress Test\")\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# We will sweep degrees up to near the interpolation threshold (N=160)\n",
    "# Step size 5 gives us 20 models to check (1, 6, 11... 96)\n",
    "degrees = range(1, 100, 5) \n",
    "lambda_fix = 0.1 # Moderate regularization\n",
    "\n",
    "print(f\"Running Degree Sweep (1 to {degrees[-1]})...\")\n",
    "print(f\"Interpolation Threshold is at p = n = {len(y_train)}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Storage\n",
    "history_deg = {\n",
    "    'degree': [],\n",
    "    'ols_test_mse': [],\n",
    "    'ridge_test_mse': [],\n",
    "    'ols_norm': [],\n",
    "    'ridge_norm': []\n",
    "}\n",
    "\n",
    "# --- 2. THE SWEEP ---\n",
    "for d in degrees:\n",
    "    # A. Construct & Scale Features\n",
    "    # We must rebuild and rescale for every degree\n",
    "    X_tr_raw = construct_vandermonde_matrix(x_train, d).numpy()\n",
    "    X_te_raw = construct_vandermonde_matrix(x_test, d).numpy()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    # Fit on training features (excluding intercept)\n",
    "    X_tr_feat = scaler.fit_transform(X_tr_raw[:, 1:])\n",
    "    X_te_feat = scaler.transform(X_te_raw[:, 1:])\n",
    "    \n",
    "    # Re-stack with intercept\n",
    "    X_tr = np.hstack([X_tr_raw[:, :1], X_tr_feat])\n",
    "    X_te = np.hstack([X_te_raw[:, :1], X_te_feat])\n",
    "    \n",
    "    # Convert to Tensor\n",
    "    X_tr_T = torch.tensor(X_tr, dtype=torch.float64)\n",
    "    X_te_T = torch.tensor(X_te, dtype=torch.float64)\n",
    "    y_tr_T = y_target # Already float64\n",
    "    # Fix: Ensure y_test is correct shape/type\n",
    "    y_te_T = y_test.to(torch.float64).view(-1, 1)\n",
    "\n",
    "    # B. Solve OLS (Unregularized)\n",
    "    # Use Pseudoinverse (pinv) because near p=N, matrix is singular\n",
    "    beta_ols = torch.linalg.pinv(X_tr_T) @ y_tr_T\n",
    "    \n",
    "    # C. Solve Ridge (Regularized)\n",
    "    # Rely on default use_cholesky=True from previous definition\n",
    "    beta_ridge = solve_ridge_analytical(X_tr_T, y_tr_T, lambda_fix)\n",
    "    \n",
    "    # D. Predict & Record\n",
    "    # OLS\n",
    "    y_ols = X_te_T @ beta_ols\n",
    "    mse_ols = torch.mean((y_ols - y_te_T)**2).item()\n",
    "    \n",
    "    # Ridge\n",
    "    y_ridge = X_te_T @ beta_ridge\n",
    "    mse_ridge = torch.mean((y_ridge - y_te_T)**2).item()\n",
    "    \n",
    "    # Store\n",
    "    history_deg['degree'].append(d)\n",
    "    history_deg['ols_test_mse'].append(mse_ols)\n",
    "    history_deg['ridge_test_mse'].append(mse_ridge)\n",
    "    history_deg['ols_norm'].append(torch.norm(beta_ols).item())\n",
    "    history_deg['ridge_norm'].append(torch.norm(beta_ridge).item())\n",
    "\n",
    "print(\"Comparison Complete.\")\n",
    "\n",
    "# --- 3. DETAILED TABLE (Expanded View) ---\n",
    "print(\"-\" * 85)\n",
    "print(f\"{'Deg':<5} | {'OLS MSE':<12} | {'Ridge MSE':<12} | {'OLS Norm':<12} | {'Ridge Norm':<12}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "# Select 10 evenly spaced indices to display\n",
    "n_points = len(degrees)\n",
    "indices = np.linspace(0, n_points - 1, num=10, dtype=int)\n",
    "\n",
    "for i in indices:\n",
    "    d_val = history_deg['degree'][i]\n",
    "    ols_mse_val = history_deg['ols_test_mse'][i]\n",
    "    ridge_mse_val = history_deg['ridge_test_mse'][i]\n",
    "    ols_norm_val = history_deg['ols_norm'][i]\n",
    "    ridge_norm_val = history_deg['ridge_norm'][i]\n",
    "    \n",
    "    # Scientific notation for large values, standard for stable ones\n",
    "    print(f\"{d_val:<5} | {ols_mse_val:<12.4f} | {ridge_mse_val:<12.4f} | {ols_norm_val:<12.1e} | {ridge_norm_val:<12.1f}\")\n",
    "\n",
    "print(\"-\" * 85)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"1. OLS Norm (Column 4) explodes (e.g., 10^10) as complexity rises, causing instability.\")\n",
    "print(\"2. Ridge Norm (Column 5) stays constrained (approx 5-10), keeping Ridge MSE stable.\")\n",
    "print(\"3. This proves that λ acts as a strict limit on the model's physical size (Norm).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The experimental results from the degree sweep provide a quantitative demonstration of the divergence between unregularized and regularized solutions as model complexity increases. This analysis addresses the underlying causes of the instability observed in the Ordinary Least Squares (OLS) solution and the specific mechanism by which Ridge Regression ($\\lambda = 0.1$) restores stability.\n",
    "\n",
    "#### 1. Why Coefficient Norms Explode\n",
    "\n",
    "The explosion of the coefficient norm ($||\\beta||_2$) is a symptom of both numerical instability and high variance of the estimator under the OLS solution.\n",
    "\n",
    "* **Geometric Cause (Near-Singularity):** As the polynomial degree ($p$) increases, the columns of the Vandermonde design matrix become highly collinear. This results in a Gram matrix ($X^T X$) that is nearly singular, characterized by a pathologically high condition number ($\\kappa \\approx 10^{12}$). Inverting a nearly singular matrix involves division by eigenvalues close to zero. To minimize the Residual Sum of Squares (RSS) on the training data, the solver generates massive coefficients with opposing signs. These large weights cancel each other out to fit the noise in the training set, a phenomenon known as _variance explosion*_.\n",
    "* **Statistical Result:** This numerical instability manifests as _overfitting_. The model memorizes the data points perfectly but fails to generalize, leading to the drastic increase in Test MSE ($183.20$ at Degree 96).\n",
    "\n",
    "#### 2. The Mechanism of Ridge Regression\n",
    "\n",
    "The fixed regularization parameter ($\\lambda = 0.1$) acts simultaneously as a statistical constraint and a numerical stabilizer. Its effectiveness is observed in the maintenance of a low Test MSE ($0.30$) and a controlled coefficient norm.\n",
    "\n",
    "* **Complexity Control:**\n",
    "    The Ridge objective function, $L(\\beta) = \\text{MSE} + \\lambda ||\\beta||_2^2$, imposes a penalty on the magnitude of the coefficient vector. By setting $\\lambda = 0.1$, the optimization problem penalizes solutions with large norms. This prevents the \"cancellation effect\" observed in OLS, forcing the model to select a solution within a stable region of the parameter space ($||\\beta||_2 \\approx 2.2$), effectively reducing the model's variance.\n",
    "\n",
    "* **Eigenvalue Shift:**\n",
    "    Mathematically, the Ridge solution involves inverting the matrix $(X^T X + \\lambda I)$. The term $\\lambda I$ adds a positive constant to the eigenvalues of $X^T X$. This relates directly to the property that shifting eigenvalues affects the positive definiteness of the matrix.\n",
    "    * **Without Regularization:** The inversion depends on $\\frac{1}{\\sigma_{min}}$. As $\\sigma_{min} \\to 0$, the term approaches infinity.\n",
    "    * **With Regularization:** The inversion becomes $\\frac{1}{\\sigma_{min} + \\lambda}$. The parameter $\\lambda$ acts as a numerical \"floor,\" ensuring the denominator is bounded away from zero.\n",
    "\n",
    "**Conclusion**: Ridge does not fix the ill-conditioning of the Vandermonde matrix itself, but it creates a well-conditioned matrix to invert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 5: Advanced Visualization (The \"Why\" and The \"What's Next\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --- 1. SANITIZATION (Fixing NumPy 2.0 Compatibility) ---\n",
    "# We ensure all targets are pure PyTorch tensors to prevent __array_wrap__ errors\n",
    "# This fixes the \"RuntimeError: mat1 and mat2 shapes cannot be multiplied\" by ensuring correct shapes\n",
    "y_target_safe = torch.as_tensor(y_train).clone().detach().to(dtype=torch.float64).view(-1, 1)\n",
    "y_test_safe = torch.as_tensor(y_test).clone().detach().to(dtype=torch.float64).view(-1, 1)\n",
    "y_target = y_train.double() \n",
    "\n",
    "# --- 2. PREPARATION ---\n",
    "# Ensure we use the same Degree 12 data from the Lambda Sweep\n",
    "DEGREE_ILL = 12\n",
    "X_raw = construct_vandermonde_matrix(x_train, DEGREE_ILL).double()\n",
    "\n",
    "# Calculate Eigenvalues of X^T X (The \"Spectrum\")\n",
    "XTX = X_raw.T @ X_raw\n",
    "eigenvalues = torch.linalg.eigvalsh(XTX).flip(0) # Sort descending\n",
    "eigenvalues_np = eigenvalues.numpy()\n",
    "\n",
    "# --- PLOT 1: The Eigenvalue Spectrum (The \"Floor\") ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(eigenvalues_np, 'k-o', label='Original Eigenvalues $\\sigma_i^2$', linewidth=2)\n",
    "ax.axhline(0, color='gray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Visualization Lambda (Large enough to see the shift)\n",
    "lambda_demo = 1e5 \n",
    "shifted_eigenvalues = eigenvalues_np + lambda_demo\n",
    "\n",
    "ax.plot(shifted_eigenvalues, 'g--x', label=f'Ridge Eigenvalues ($\\sigma_i^2 + 10^5$)')\n",
    "ax.fill_between(range(len(eigenvalues_np)), eigenvalues_np, shifted_eigenvalues, color='green', alpha=0.1, label='The \"Lift\" (Stabilization)')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('1. The Mechanism: Eigenvalue Shift', fontsize=14)\n",
    "ax.set_ylabel('Eigenvalue Magnitude (Log Scale)')\n",
    "ax.set_xlabel('Eigenvalue Index (0=Largest, 12=Smallest)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.text(5, 1e-5, 'The \"Cliff\"\\n(Near-Zero Eigenvalues)', color='red', fontsize=10, ha='center')\n",
    "\n",
    "\n",
    "# --- PLOT 2: Effective Degrees of Freedom (Corrected & Rigorous) ---\n",
    "def compute_effective_df(eigvals, lam):\n",
    "    return np.sum(eigvals / (eigvals + lam))\n",
    "\n",
    "# 1. Calculate DF and MSE over a wide lambda range for the curve\n",
    "lambda_sweep = torch.logspace(-5, 15, 100, dtype=torch.float64)\n",
    "dfs = [compute_effective_df(eigenvalues_np, lam.item()) for lam in lambda_sweep]\n",
    "\n",
    "# We need to compute MSE for each lambda to plot the curve\n",
    "mse_sweep = []\n",
    "# We need X_test constructed for prediction inside the loop\n",
    "X_test_raw = construct_vandermonde_matrix(x_test, DEGREE_ILL).double()\n",
    "\n",
    "for lam in lambda_sweep:\n",
    "    beta = solve_ridge_analytical(X_raw, y_target_safe, lam.item()) \n",
    "    y_pred = X_test_raw @ beta \n",
    "    mse_sweep.append(compute_mse(y_test_safe, y_pred))\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(dfs, mse_sweep, 'b-', linewidth=2, label='Test MSE Curve')\n",
    "\n",
    "# 2. Mark the Optimum\n",
    "min_mse_val = min(mse_sweep)\n",
    "min_idx = np.argmin(mse_sweep)\n",
    "opt_df = dfs[min_idx]\n",
    "opt_lambda = lambda_sweep[min_idx]\n",
    "\n",
    "ax.plot(opt_df, min_mse_val, 'r*', markersize=15, label=f'Optimal Model\\n(df={opt_df:.1f})')\n",
    "\n",
    "# 3. Mark OLS (Lambda -> 0, df -> 13)\n",
    "ols_df = dfs[0] \n",
    "ols_mse = mse_sweep[0]\n",
    "ax.plot(ols_df, ols_mse, 'gx', markersize=8, label='OLS Limit ($\\lambda \\\\to 0$)')\n",
    "\n",
    "# 4. Formatting (Standard Orientation: Simple -> Complex)\n",
    "ax.set_xlim(-0.5, 13.5) \n",
    "ax.set_xlabel('Effective Degrees of Freedom ($df$)\\n(0=Null $\\\\to$ 13=OLS)')\n",
    "ax.set_ylabel('Test MSE')\n",
    "ax.set_title('2. Complexity Control: Bias-Variance Tradeoff', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Secondary Axis for Lambda (Top)\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xlim(ax.get_xlim())\n",
    "ticks_lambda = [1e10, 1e5, 1e0, 1e-5]\n",
    "ticks_df = [compute_effective_df(eigenvalues_np, l) for l in ticks_lambda]\n",
    "ax2.set_xticks(ticks_df)\n",
    "ax2.set_xticklabels([f'$10^{{{np.log10(l):.0f}}}$' for l in ticks_lambda])\n",
    "ax2.set_xlabel('Regularization Strength $\\lambda$ (Inverse Complexity)')\n",
    "\n",
    "# 6. Context Annotation (Addressing the \"Suspicious Optimum\")\n",
    "if opt_df > 12:\n",
    "    ax.text(0.5, 0.5, \"Note: OLS is optimal here because N >> p\\n(Safe Regime)\", \n",
    "            transform=ax.transAxes, ha='center', bbox=dict(boxstyle=\"round\", fc=\"w\"))\n",
    "\n",
    "\n",
    "# --- PLOT 3: The Descent Check (Preview of Part 5) ---\n",
    "# Goal: Show how Regularization SUPPRESSES the singularity peak\n",
    "print(\"Running Extended Degree Sweep (Preview of Part 5)...\")\n",
    "degrees_extended = range(100, 250, 5) \n",
    "\n",
    "ols_preview = []\n",
    "ridge_weak_preview = []   # lambda = 0.01\n",
    "ridge_strong_preview = [] # lambda = 100.0\n",
    "\n",
    "for d in degrees_extended:\n",
    "    X_tr = construct_vandermonde_matrix(x_train, d).double()\n",
    "    X_te = construct_vandermonde_matrix(x_test, d).double()\n",
    "    \n",
    "    # 1. OLS (Pinv)\n",
    "    beta_ols = torch.linalg.pinv(X_tr) @ y_target_safe\n",
    "    ols_preview.append(compute_mse(y_test_safe, X_te @ beta_ols))\n",
    "    \n",
    "    # 2. Weak Ridge\n",
    "    beta_weak = solve_ridge_analytical(X_tr, y_target_safe, 0.01)\n",
    "    ridge_weak_preview.append(compute_mse(y_test_safe, X_te @ beta_weak))\n",
    "\n",
    "    # 3. Strong Ridge\n",
    "    beta_strong = solve_ridge_analytical(X_tr, y_target_safe, 100.0)\n",
    "    ridge_strong_preview.append(compute_mse(y_test_safe, X_te @ beta_strong))\n",
    "\n",
    "ax = axes[2]\n",
    "ax.axvline(160, color='k', linestyle='--', label='Interpolation Threshold ($p=n$)')\n",
    "\n",
    "ax.plot(degrees_extended, ols_preview, 'r-', linewidth=2, label='Unregularized OLS')\n",
    "ax.plot(degrees_extended, ridge_weak_preview, 'b--', label='Weak Ridge ($\\lambda=0.01$)')\n",
    "ax.plot(degrees_extended, ridge_strong_preview, 'g-o', linewidth=2, label='Strong Ridge ($\\lambda=100$)')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('3. The Preview: Taming the Singularity', fontsize=14)\n",
    "ax.set_xlabel('Polynomial Degree ($p$)')\n",
    "ax.set_ylabel('Test MSE (Log Scale)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax.annotate('Strong $\\lambda$ suppresses\\nthe peak', \n",
    "            xy=(160, min(ridge_strong_preview)), \n",
    "            xytext=(200, max(ridge_strong_preview)),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 5: Advanced Visualization (The \"Why\" and The \"What's Next\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --- 1. SANITIZATION (Fixing NumPy 2.0 Compatibility) ---\n",
    "# We ensure all targets are pure PyTorch tensors to prevent __array_wrap__ errors\n",
    "# This fixes the \"RuntimeError: mat1 and mat2 shapes cannot be multiplied\" by ensuring correct shapes\n",
    "y_target_safe = torch.as_tensor(y_train).clone().detach().to(dtype=torch.float64).view(-1, 1)\n",
    "y_test_safe = torch.as_tensor(y_test).clone().detach().to(dtype=torch.float64).view(-1, 1)\n",
    "y_target = y_train.double() \n",
    "\n",
    "# --- 2. PREPARATION ---\n",
    "# Ensure we use the same Degree 12 data from the Lambda Sweep\n",
    "DEGREE_ILL = 12\n",
    "X_raw = construct_vandermonde_matrix(x_train, DEGREE_ILL).double()\n",
    "\n",
    "# Calculate Eigenvalues of X^T X (The \"Spectrum\")\n",
    "XTX = X_raw.T @ X_raw\n",
    "eigenvalues = torch.linalg.eigvalsh(XTX).flip(0) # Sort descending\n",
    "eigenvalues_np = eigenvalues.numpy()\n",
    "\n",
    "# --- PLOT 1: The Eigenvalue Spectrum (The \"Floor\") ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(eigenvalues_np, 'k-o', label='Original Eigenvalues $\\sigma_i^2$', linewidth=2)\n",
    "ax.axhline(0, color='gray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Visualization Lambda (Large enough to see the shift)\n",
    "lambda_demo = 1e5 \n",
    "shifted_eigenvalues = eigenvalues_np + lambda_demo\n",
    "\n",
    "ax.plot(shifted_eigenvalues, 'g--x', label=f'Ridge Eigenvalues ($\\sigma_i^2 + 10^5$)')\n",
    "ax.fill_between(range(len(eigenvalues_np)), eigenvalues_np, shifted_eigenvalues, color='green', alpha=0.1, label='The \"Lift\" (Stabilization)')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('1. The Mechanism: Eigenvalue Shift', fontsize=14)\n",
    "ax.set_ylabel('Eigenvalue Magnitude (Log Scale)')\n",
    "ax.set_xlabel('Eigenvalue Index (0=Largest, 12=Smallest)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.text(5, 1e-5, 'The \"Cliff\"\\n(Near-Zero Eigenvalues)', color='red', fontsize=10, ha='center')\n",
    "\n",
    "\n",
    "# --- PLOT 2: Effective Degrees of Freedom (Corrected & Rigorous) ---\n",
    "def compute_effective_df(eigvals, lam):\n",
    "    return np.sum(eigvals / (eigvals + lam))\n",
    "\n",
    "# 1. Calculate DF and MSE over a wide lambda range for the curve\n",
    "lambda_sweep = torch.logspace(-5, 15, 100, dtype=torch.float64)\n",
    "dfs = [compute_effective_df(eigenvalues_np, lam.item()) for lam in lambda_sweep]\n",
    "\n",
    "# We need to compute MSE for each lambda to plot the curve\n",
    "mse_sweep = []\n",
    "# We need X_test constructed for prediction inside the loop\n",
    "X_test_raw = construct_vandermonde_matrix(x_test, DEGREE_ILL).double()\n",
    "\n",
    "for lam in lambda_sweep:\n",
    "    beta = solve_ridge_analytical(X_raw, y_target_safe, lam.item()) \n",
    "    y_pred = X_test_raw @ beta \n",
    "    mse_sweep.append(compute_mse(y_test_safe, y_pred))\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(dfs, mse_sweep, 'b-', linewidth=2, label='Test MSE Curve')\n",
    "\n",
    "# 2. Mark the Optimum\n",
    "min_mse_val = min(mse_sweep)\n",
    "min_idx = np.argmin(mse_sweep)\n",
    "opt_df = dfs[min_idx]\n",
    "opt_lambda = lambda_sweep[min_idx]\n",
    "\n",
    "ax.plot(opt_df, min_mse_val, 'r*', markersize=15, label=f'Optimal Model\\n(df={opt_df:.1f})')\n",
    "\n",
    "# 3. Mark OLS (Lambda -> 0, df -> 13)\n",
    "ols_df = dfs[0] \n",
    "ols_mse = mse_sweep[0]\n",
    "ax.plot(ols_df, ols_mse, 'gx', markersize=8, label='OLS Limit ($\\lambda \\\\to 0$)')\n",
    "\n",
    "# 4. Formatting (Standard Orientation: Simple -> Complex)\n",
    "ax.set_xlim(-0.5, 13.5) \n",
    "ax.set_xlabel('Effective Degrees of Freedom ($df$)\\n(0=Null $\\\\to$ 13=OLS)')\n",
    "ax.set_ylabel('Test MSE')\n",
    "ax.set_title('2. Complexity Control: Bias-Variance Tradeoff', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Secondary Axis for Lambda (Top)\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xlim(ax.get_xlim())\n",
    "ticks_lambda = [1e10, 1e5, 1e0, 1e-5]\n",
    "ticks_df = [compute_effective_df(eigenvalues_np, l) for l in ticks_lambda]\n",
    "ax2.set_xticks(ticks_df)\n",
    "ax2.set_xticklabels([f'$10^{{{np.log10(l):.0f}}}$' for l in ticks_lambda])\n",
    "ax2.set_xlabel('Regularization Strength $\\lambda$ (Inverse Complexity)')\n",
    "\n",
    "# 6. Context Annotation (Addressing the \"Suspicious Optimum\")\n",
    "if opt_df > 12:\n",
    "    ax.text(0.5, 0.5, \"Note: OLS is optimal here because N >> p\\n(Safe Regime)\", \n",
    "            transform=ax.transAxes, ha='center', bbox=dict(boxstyle=\"round\", fc=\"w\"))\n",
    "\n",
    "\n",
    "# --- PLOT 3: The Descent Check (Preview of Part 5) ---\n",
    "# Goal: Show how Regularization SUPPRESSES the singularity peak\n",
    "print(\"Running Extended Degree Sweep (Preview of Part 5)...\")\n",
    "degrees_extended = range(100, 250, 5) \n",
    "\n",
    "ols_preview = []\n",
    "ridge_weak_preview = []   # lambda = 0.01\n",
    "ridge_strong_preview = [] # lambda = 100.0\n",
    "\n",
    "for d in degrees_extended:\n",
    "    X_tr = construct_vandermonde_matrix(x_train, d).double()\n",
    "    X_te = construct_vandermonde_matrix(x_test, d).double()\n",
    "    \n",
    "    # 1. OLS (Pinv)\n",
    "    beta_ols = torch.linalg.pinv(X_tr) @ y_target_safe\n",
    "    ols_preview.append(compute_mse(y_test_safe, X_te @ beta_ols))\n",
    "    \n",
    "    # 2. Weak Ridge\n",
    "    beta_weak = solve_ridge_analytical(X_tr, y_target_safe, 0.01)\n",
    "    ridge_weak_preview.append(compute_mse(y_test_safe, X_te @ beta_weak))\n",
    "\n",
    "    # 3. Strong Ridge\n",
    "    beta_strong = solve_ridge_analytical(X_tr, y_target_safe, 100.0)\n",
    "    ridge_strong_preview.append(compute_mse(y_test_safe, X_te @ beta_strong))\n",
    "\n",
    "ax = axes[2]\n",
    "ax.axvline(160, color='k', linestyle='--', label='Interpolation Threshold ($p=n$)')\n",
    "\n",
    "ax.plot(degrees_extended, ols_preview, 'r-', linewidth=2, label='Unregularized OLS')\n",
    "ax.plot(degrees_extended, ridge_weak_preview, 'b--', label='Weak Ridge ($\\lambda=0.01$)')\n",
    "ax.plot(degrees_extended, ridge_strong_preview, 'g-o', linewidth=2, label='Strong Ridge ($\\lambda=100$)')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('3. The Preview: Taming the Singularity', fontsize=14)\n",
    "ax.set_xlabel('Polynomial Degree ($p$)')\n",
    "ax.set_ylabel('Test MSE (Log Scale)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax.annotate('Strong $\\lambda$ suppresses\\nthe peak', \n",
    "            xy=(160, min(ridge_strong_preview)), \n",
    "            xytext=(200, max(ridge_strong_preview)),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 5: Advanced Visualization (Ridge vs OLS Educational Plots)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --- Helper Functions (Ensure they exist) ---\n",
    "def construct_vandermonde_matrix(x, degree):\n",
    "    \"\"\"Constructs the Vandermonde matrix for polynomial regression.\"\"\"\n",
    "    X = torch.stack([x ** i for i in range(degree + 1)], dim=1)\n",
    "    return X\n",
    "\n",
    "def solve_ridge_analytical(X, y, lam):\n",
    "    \"\"\"Analytical Ridge solution: (XᵀX + λI)⁻¹ Xᵀy\"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    I = torch.eye(n_features, dtype=X.dtype)\n",
    "    return torch.linalg.solve(X.T @ X + lam * I, X.T @ y)\n",
    "\n",
    "def compute_mse(y_true, y_pred):\n",
    "    return torch.mean((y_true - y_pred) ** 2).item()\n",
    "\n",
    "# --- 1. SANITIZATION ---\n",
    "y_target_safe = torch.as_tensor(y_train).clone().detach().to(dtype=torch.float64).view(-1, 1)\n",
    "y_test_safe = torch.as_tensor(y_test).clone().detach().to(dtype=torch.float64).view(-1, 1)\n",
    "\n",
    "# --- 2. PREPARATION ---\n",
    "DEGREE_ILL = 12\n",
    "X_raw = construct_vandermonde_matrix(x_train, DEGREE_ILL).double()\n",
    "X_test_raw = construct_vandermonde_matrix(x_test, DEGREE_ILL).double()\n",
    "\n",
    "# Eigenvalue spectrum\n",
    "XTX = X_raw.T @ X_raw\n",
    "eigenvalues = torch.linalg.eigvalsh(XTX).flip(0)  # descending order\n",
    "eigenvalues_np = eigenvalues.numpy()\n",
    "\n",
    "# --- PLOT SETUP ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# PLOT 1: Eigenvalue Spectrum — “The Floor”\n",
    "# -------------------------------------------------------------------------\n",
    "ax = axes[0]\n",
    "ax.plot(eigenvalues_np, 'ko-', label=r'Original Eigenvalues $\\sigma_i^2$', linewidth=2)\n",
    "lambda_demo = 1e5\n",
    "shifted_eigenvalues = eigenvalues_np + lambda_demo\n",
    "ax.plot(shifted_eigenvalues, 'g--x', label=r'Ridge Eigenvalues $(\\sigma_i^2 + 10^5)$')\n",
    "ax.fill_between(range(len(eigenvalues_np)), eigenvalues_np, shifted_eigenvalues,\n",
    "                color='green', alpha=0.15, label='“Lift” (Stabilization)')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('1. The Mechanism: Eigenvalue Shift', fontsize=14)\n",
    "ax.set_ylabel('Eigenvalue Magnitude (log scale)')\n",
    "ax.set_xlabel('Eigenvalue Index (0 = largest, 12 = smallest)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.text(5, 1e-5, 'The “Cliff” (Near-Zero Eigenvalues)', color='red', fontsize=10, ha='center')\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# PLOT 2: Effective Degrees of Freedom — “The Bias-Variance Tradeoff”\n",
    "# -------------------------------------------------------------------------\n",
    "def compute_effective_df(eigvals, lam):\n",
    "    return np.sum(eigvals / (eigvals + lam))\n",
    "\n",
    "lambda_sweep = torch.logspace(-5, 15, 100, dtype=torch.float64)\n",
    "dfs = [compute_effective_df(eigenvalues_np, lam.item()) for lam in lambda_sweep]\n",
    "\n",
    "mse_sweep = []\n",
    "for lam in lambda_sweep:\n",
    "    beta = solve_ridge_analytical(X_raw, y_target_safe, lam.item())\n",
    "    y_pred = X_test_raw @ beta\n",
    "    mse_sweep.append(compute_mse(y_test_safe, y_pred))\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(dfs, mse_sweep, 'b-', linewidth=2, label='Test MSE Curve')\n",
    "\n",
    "min_idx = np.argmin(mse_sweep)\n",
    "ax.plot(dfs[min_idx], mse_sweep[min_idx], 'r*', markersize=15,\n",
    "        label=f'Optimal Ridge Model\\n(df={dfs[min_idx]:.1f})')\n",
    "\n",
    "# Mark OLS limit\n",
    "ax.plot(dfs[0], mse_sweep[0], 'gx', markersize=10, label='OLS Limit ($\\\\lambda \\\\to 0$)')\n",
    "\n",
    "ax.set_xlim(-0.5, 13.5)\n",
    "ax.set_xlabel(r'Effective Degrees of Freedom ($df$)')\n",
    "ax.set_ylabel('Test MSE')\n",
    "ax.set_title('2. Complexity Control: Bias–Variance Tradeoff', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add λ scale on top\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xlim(ax.get_xlim())\n",
    "ticks_lambda = [1e10, 1e5, 1e0, 1e-5]\n",
    "ticks_df = [compute_effective_df(eigenvalues_np, l) for l in ticks_lambda]\n",
    "ax2.set_xticks(ticks_df)\n",
    "ax2.set_xticklabels([f'$10^{{{int(np.log10(l))}}}$' for l in ticks_lambda])\n",
    "ax2.set_xlabel(r'Regularization Strength $\\lambda$ (Inverse Complexity)')\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# PLOT 3: Degree Sweep — “Taming the Singularity”\n",
    "# -------------------------------------------------------------------------\n",
    "degrees_extended = range(100, 250, 5)\n",
    "ols_preview, ridge_weak_preview, ridge_strong_preview = [], [], []\n",
    "\n",
    "for d in degrees_extended:\n",
    "    X_tr = construct_vandermonde_matrix(x_train, d).double()\n",
    "    X_te = construct_vandermonde_matrix(x_test, d).double()\n",
    "\n",
    "    beta_ols = torch.linalg.pinv(X_tr) @ y_target_safe\n",
    "    beta_weak = solve_ridge_analytical(X_tr, y_target_safe, 0.01)\n",
    "    beta_strong = solve_ridge_analytical(X_tr, y_target_safe, 100.0)\n",
    "\n",
    "    ols_preview.append(compute_mse(y_test_safe, X_te @ beta_ols))\n",
    "    ridge_weak_preview.append(compute_mse(y_test_safe, X_te @ beta_weak))\n",
    "    ridge_strong_preview.append(compute_mse(y_test_safe, X_te @ beta_strong))\n",
    "\n",
    "ax = axes[2]\n",
    "ax.plot(degrees_extended, ols_preview, 'r-', linewidth=2, label='Unregularized OLS')\n",
    "ax.plot(degrees_extended, ridge_weak_preview, 'b--', linewidth=2, label='Weak Ridge ($\\\\lambda=0.01$)')\n",
    "ax.plot(degrees_extended, ridge_strong_preview, 'g-o', linewidth=2, label='Strong Ridge ($\\\\lambda=100$)')\n",
    "ax.axvline(160, color='k', linestyle='--', label='Interpolation Threshold ($p=n$)')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('3. The Preview: Taming the Singularity', fontsize=14)\n",
    "ax.set_xlabel('Polynomial Degree ($p$)')\n",
    "ax.set_ylabel('Test MSE (log scale)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax.annotate('Strong λ suppresses\\nthe peak',\n",
    "            xy=(160, min(ridge_strong_preview)),\n",
    "            xytext=(200, max(ridge_strong_preview)),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Without Feature Scaling\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURATION ---\n",
    "# We use the degree 12 polynomial to demonstrate the \"cliff\" geometry\n",
    "DEGREE_ILL = 12\n",
    "# Force Double Precision to avoid NaNs with these massive numbers\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Re-construct matrices (assuming x_train, y_train exist from Part 1)\n",
    "X = construct_vandermonde_matrix(x_train, DEGREE_ILL).double()\n",
    "y_target = y_train.double()\n",
    "n_features = X.shape[1]\n",
    "\n",
    "print(f\"Optimization Problem Setup:\")\n",
    "print(f\"  Parameters (p): {n_features}\")\n",
    "print(f\"  Data Points (n): {X.shape[0]}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- 2. The Modern Approach: Autograd ---\n",
    "\n",
    "def compute_loss_autograd(X, y, beta, lambda_):\n",
    "    \"\"\"\n",
    "    Computes Regularized Loss: L(β) = 0.5*SSE + 0.5*λ*||β||^2\n",
    "    \"\"\"\n",
    "    # 1. Prediction (Standard Matrix Multiplication)\n",
    "    y_pred = X @ beta \n",
    "    \n",
    "    # 2. Loss Components\n",
    "    sse = 0.5 * torch.sum((y_pred - y)**2)\n",
    "    l2_penalty = 0.5 * lambda_ * torch.sum(beta**2)\n",
    "    \n",
    "    return sse + l2_penalty\n",
    "\n",
    "# --- 3. Execution & Diagnostics ---\n",
    "\n",
    "# Initialize random weights with gradient tracking enabled\n",
    "beta_init = torch.randn(n_features, 1, requires_grad=True)\n",
    "lambda_val = 1.0\n",
    "\n",
    "# Forward Pass\n",
    "loss = compute_loss_autograd(X, y_target, beta_init, lambda_val)\n",
    "\n",
    "# Backward Pass (The Magic of Autograd)\n",
    "loss.backward()\n",
    "grad = beta_init.grad\n",
    "\n",
    "# Diagnostics\n",
    "sv = torch.linalg.svdvals(X)\n",
    "cond_X = (sv[0] / sv[-1]).item()\n",
    "\n",
    "print(f\"Condition Number (X): {cond_X:.2e}\")\n",
    "print(f\"Initial Loss:         {loss.item():.2e}\")\n",
    "print(f\"Initial Gradient Norm:{torch.norm(grad).item():.2e}\")\n",
    "\n",
    "print(\"-\" *  60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(42)\n",
    "DEGREE_ILL = 12 \n",
    "\n",
    "# --- 2. DATA SETUP ---\n",
    "# Construct Raw Vandermonde Matrix\n",
    "X_raw = construct_vandermonde_matrix(x_train, DEGREE_ILL).numpy()\n",
    "y_target = y_train.numpy()\n",
    "\n",
    "# --- 3. FEATURE SCALING (Protecting the Intercept) ---\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Split Intercept (Col 0) from Features (Cols 1-12)\n",
    "X_features = X_raw[:, 1:] \n",
    "\n",
    "# Scale only the features\n",
    "X_features_scaled = scaler.fit_transform(X_features)\n",
    "\n",
    "# Re-attach the intercept column (all 1s)\n",
    "X_scaled = np.hstack([X_raw[:, :1], X_features_scaled])\n",
    "\n",
    "# Convert to Tensor for Diagnostic Calculation\n",
    "X_torch = torch.tensor(X_scaled)\n",
    "y_torch = torch.tensor(y_target)\n",
    "\n",
    "# --- 4. DIAGNOSTICS (Calculating the Metrics) ---\n",
    "# We use the same logic as the \"Raw\" block to ensure a fair comparison\n",
    "\n",
    "# Initialize random weights\n",
    "n_features = X_torch.shape[1]\n",
    "beta_init = torch.randn(n_features, 1, requires_grad=True)\n",
    "lambda_val = 1.0\n",
    "\n",
    "# Calculate Loss & Gradient (using Autograd)\n",
    "# Loss = 0.5*SSE + 0.5*lambda*L2\n",
    "y_pred = X_torch @ beta_init\n",
    "sse = 0.5 * torch.sum((y_pred - y_torch)**2)\n",
    "l2 = 0.5 * lambda_val * torch.sum(beta_init**2)\n",
    "loss = sse + l2\n",
    "\n",
    "loss.backward()\n",
    "grad = beta_init.grad\n",
    "\n",
    "# Calculate Condition Number\n",
    "cond_X = np.linalg.cond(X_scaled)\n",
    "\n",
    "# --- 5. OUTPUT (Matching Original Format) ---\n",
    "print(f\"Optimization Problem Setup (Scaled Data):\")\n",
    "print(f\"  Parameters (p): {n_features}\")\n",
    "print(f\"  Data Points (n): {X_torch.shape[0]}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Condition Number (X): {cond_X:.2e}\")\n",
    "print(f\"Initial Loss:         {loss.item():.2e}\")\n",
    "print(f\"Initial Gradient Norm:{torch.norm(grad).item():.2e}\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Implement Ridge Analytical Solver ---\n",
    "\n",
    "def solve_ridge_analytical(X, y, lambda_, use_cholesky=True):\n",
    "    \"\"\"\n",
    "    Solves Ridge Regression analytically: β = (XᵀX + λI)⁻¹ Xᵀy\n",
    "        \n",
    "    Since A = (XᵀX + λI) is Symmetric Positive Definite (SPD) for λ > 0,\n",
    "    we can use Cholesky Decomposition for a numerically stable solution.\n",
    "    \"\"\"\n",
    "    # 1. Shape Safety\n",
    "    if y.ndim == 1: y = y.view(-1, 1)\n",
    "    n, d = X.shape\n",
    "\n",
    "    # 2. Construct the Linear System Ax = b\n",
    "    # We use standard matrix multiplication (@) for clarity and speed here\n",
    "    XTX = X.T @ X\n",
    "    XTy = X.T @ y\n",
    "    I = torch.eye(d, dtype=X.dtype, device=X.device)\n",
    "    A = XTX + (lambda_ * I)\n",
    "\n",
    "    # 3. Solve\n",
    "    # Try Cholesky (Fast/Stable for SPD), fall back to LU if numerical noise breaks PD\n",
    "    if use_cholesky and lambda_ > 1e-6:\n",
    "        try:\n",
    "            L = torch.linalg.cholesky(A)\n",
    "            beta = torch.cholesky_solve(XTy, L)\n",
    "        except RuntimeError:\n",
    "            # Fallback if lambda is too small to guarantee PD in float precision\n",
    "            beta = torch.linalg.solve(A, XTy)\n",
    "    else:\n",
    "        beta = torch.linalg.solve(A, XTy)\n",
    "\n",
    "    # 4. Compute Condition Number (Diagnostics)\n",
    "    # Always use Double Precision (float64) for Condition Number to avoid overflow/inf\n",
    "    cond_number = torch.linalg.cond(A.double()).item()\n",
    "\n",
    "    return beta, cond_number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_stability_results(cond_ols, cond_ridge, norm_ols, norm_ridge, lambda_val):\n",
    "    \"\"\"\n",
    "    Dynamically interprets the results of the Ridge Regression stability analysis.\n",
    "    \"\"\"\n",
    "    improvement_factor = cond_ols / cond_ridge\n",
    "    norm_shrinkage = norm_ols / norm_ridge\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INTERPRETATION\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # --- 1. Analysis of Geometric Curvature (Condition Number) ---\n",
    "    print(f\"1. Geometry & Eigenvalue Shift:\")\n",
    "    if improvement_factor > 100:\n",
    "        print(f\"   - RESULT: Massive Stabilization. The condition number improved by {improvement_factor:.1e}x.\")\n",
    "        print(f\"   - THEORY: The matrix X^T X was pathologically ill-conditioned (squashed valley).\")\n",
    "        print(f\"   - MECHANISM: Adding {lambda_val:.0e}*I shifted the smallest eigenvalues away from zero:\")\n",
    "        print(f\"     λ_min(A) = λ_min(X^T X) + {lambda_val:.0e} >> 0.\")\n",
    "        print(f\"   - CONSEQUENCE: The Hessian is now strictly Positive Definite.\")\n",
    "    elif improvement_factor > 1.5:\n",
    "        print(f\"   - RESULT: Moderate Stabilization ({improvement_factor:.1f}x improvement).\")\n",
    "        print(f\"   - THEORY: The original system was somewhat stable, but Ridge made it 'rounder'.\")\n",
    "    else:\n",
    "        print(f\"   - RESULT: Negligible change ({improvement_factor:.2f}x).\")\n",
    "        print(f\"   - CONTEXT: The original system was already well-conditioned (likely low degree).\")\n",
    "        print(f\"     Regularization has little geometric effect on spherical bowls.\")\n",
    "\n",
    "    # --- 2. Analysis of Solution Complexity (Norm) ---\n",
    "    print(f\"\\n2. Complexity & Constraints:\")\n",
    "    if norm_shrinkage > 2.0:\n",
    "        print(f\"   - RESULT: Strong Regularization. The coefficient norm shrunk by {norm_shrinkage:.1f}x.\")\n",
    "        print(f\"   - INTERPRETATION: The penalty λ||β||² successfully constrained the search space.\")\n",
    "        print(f\"   - EFFECT: Variance is reduced; the model is less sensitive to noise.\")\n",
    "    elif norm_shrinkage > 1.0:\n",
    "        print(f\"   - RESULT: Mild Shrinkage. Norm reduced by {norm_shrinkage:.2f}x.\")\n",
    "    else:\n",
    "        print(f\"   - RESULT: No Shrinkage. The OLS solution was already within the constraint region.\")\n",
    "\n",
    "    # # --- 3. Conclusion on Uniqueness ---\n",
    "    # print(f\"\\n3. Global Optimality:\")\n",
    "    # if cond_ridge < 1e12:\n",
    "    #     print(f\"   - CONCLUSION: The Regularized Hessian is sufficiently conditioned.\")\n",
    "    #     print(f\"   - VERDICT: The solution β_ridge is the UNIQUE global minimizer.\")\n",
    "    # else:\n",
    "    #     # This catches cases where Degree > 15 and even Ridge struggles with unscaled data\n",
    "    #     print(f\"   - WARNING: Even with Ridge, the condition number remains high ({cond_ridge:.1e}).\")\n",
    "    #     print(f\"   - REASON: The unscaled polynomial features (x^12) create scale disparities > 10^12.\")\n",
    "    #     print(f\"   - FIX: In production, feature scaling (StandardScaler) is required before Ridge.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# --- Ridge Regression Stability Analysis ---\n",
    "# 1. Compute Baseline (Unregularized / OLS)\n",
    "# We compute this in Double Precision to get the 'True' terrible condition number\n",
    "cond_ols = torch.linalg.cond((X.T @ X).double()).item()\n",
    "\n",
    "# 2. Compute Ridge with Small vs Large Lambda\n",
    "lambda_small = 1e-10\n",
    "lambda_large = 1e2\n",
    "\n",
    "# 1. Solve (Reuse your previous solver calls)\n",
    "# We calculate these explicitly to ensure they are fresh\n",
    "cond_ols_val = torch.linalg.cond((X.T @ X).double()).item()\n",
    "beta_small, cond_small_val = solve_ridge_analytical(X, y_train, lambda_small, use_cholesky=True)\n",
    "beta_large, cond_large_val = solve_ridge_analytical(X, y_train, lambda_large, use_cholesky=True)\n",
    "\n",
    "# 3. Print Table (The raw data)\n",
    "print(f\"{'Metric':<25} | {'OLS (λ≈0)':<15} | {f'Ridge (λ={lambda_large:.0e})':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Condition Number':<25} | {cond_ols_val:<15.2e} | {cond_large_val:<15.2e}\")\n",
    "print(f\"{'||β|| (Norm)':<25} | {torch.norm(beta_small).item():<15.2f} | {torch.norm(beta_large).item():<15.5f}\")\n",
    "\n",
    "# 4. Run Dynamic Interpretation\n",
    "analyze_stability_results(cond_ols_val, cond_large_val, \n",
    "                          torch.norm(beta_small).item(), \n",
    "                          torch.norm(beta_large).item(), \n",
    "                          lambda_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory: The Bias-Variance Tradeoff in $\\lambda$\n",
    "\n",
    "| $\\lambda$ | Model Complexity | $\\|\\beta\\|_2$ | Bias | Variance | Effect |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---|\n",
    "| $\\lambda \\to 0$ | High | Large | Low | High | Overfitting (approaches OLS) |\n",
    "| $\\lambda \\to \\infty$ | Low | Small | High | Low | Underfitting (trivial solution) |\n",
    "| $\\lambda_{\\text{opt}}$ | Optimal | Medium | Balanced | Balanced | Generalization |\n",
    "\n",
    "### Lambda Sweep (Fixed Degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE BLOCK 3A: Sweeping Lambda with Comparative Verification\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# --- 1. CONFIGURATION & SETUP ---\n",
    "# Use Degree 12 to ensure we are stressing the numerics\n",
    "DEGREE_ILL = 12 \n",
    "# Define Lambda Range (Logarithmic: 10^-2 to 10^10) to capture the U-Curve\n",
    "lambda_values = torch.logspace(-2, 10, 50) \n",
    "\n",
    "# Re-construct matrices to ensure fresh state (Degree 12)\n",
    "# We create both PyTorch and Numpy versions for the comparison\n",
    "X = construct_vandermonde_matrix(x_train, DEGREE_ILL)\n",
    "X_test = construct_vandermonde_matrix(x_test, DEGREE_ILL)\n",
    "y_train_np = y_train.numpy() # Helper for sklearn\n",
    "\n",
    "# --- 2. Initialize Storage ---\n",
    "train_mses = []\n",
    "test_mses = []\n",
    "beta_norms = []\n",
    "condition_numbers = []\n",
    "max_discrepancy = 0.0 # Track the maximum difference between methods\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Running Lambda Sweep\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Log(λ)':<10} | {'Condition No.':<15} | {'Beta Norm':<12} | {'Method Diff':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# --- 3. The Comparative Loop ---\n",
    "for lambda_val in lambda_values:\n",
    "    lam = lambda_val.item()\n",
    "    \n",
    "    # --- METHOD A: MANUAL PYTORCH (The \"White Box\") ---\n",
    "    # Uses your manually derived formula: β = (X'X + λI)^-1 X'y\n",
    "    beta_manual, cond_num = solve_ridge_analytical(X, y_train, lam, use_cholesky=True)\n",
    "    \n",
    "    # --- METHOD B: SCIKIT-LEARN (The \"Black Box\") ---\n",
    "    # Uses standard library solver (Cholesky)\n",
    "    # fit_intercept=False because our Vandermonde matrix already has the x^0 column\n",
    "    clf = Ridge(alpha=lam, fit_intercept=False, solver='cholesky', tol=1e-12)\n",
    "    clf.fit(X.numpy(), y_train_np)\n",
    "    \n",
    "    # Align shapes: Sklearn returns (1, D) or (D,), we want (D, 1)\n",
    "    beta_sklearn = torch.tensor(clf.coef_, dtype=torch.float64).view(-1, 1)\n",
    "    if beta_sklearn.shape[0] != beta_manual.shape[0]: \n",
    "        beta_sklearn = beta_sklearn.T.view(-1, 1)\n",
    "\n",
    "    # --- COMPARISON ---\n",
    "    # Calculate vector distance between the two solutions\n",
    "    diff = torch.norm(beta_manual - beta_sklearn).item()\n",
    "    if diff > max_discrepancy: max_discrepancy = diff\n",
    "    \n",
    "    # --- PREDICTION & METRICS (Using Manual Beta) ---\n",
    "    y_train_pred = X @ beta_manual\n",
    "    y_test_pred = X_test @ beta_manual\n",
    "    \n",
    "    train_mses.append(compute_mse(y_train, y_train_pred))\n",
    "    test_mses.append(compute_mse(y_test, y_test_pred))\n",
    "    beta_norms.append(torch.norm(beta_manual).item())\n",
    "    condition_numbers.append(cond_num)\n",
    "    \n",
    "    # Print progress for specific points to keep log clean\n",
    "    if lam < 1e-1 or (1.0 < lam < 2.0) or lam > 1e9:\n",
    "        print(f\"{np.log10(lam):<10.1f} | {cond_num:<15.2e} | {beta_norms[-1]:<12.5f} | {diff:.2e}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Max Discrepancy across all runs: {max_discrepancy:.2e}\")\n",
    "\n",
    "if max_discrepancy < 1e-7:\n",
    "    print(\"SUCCESS\")\n",
    "else:\n",
    "    print(\"WARNING: Significant deviation detected. Check precision/logic.\")\n",
    "\n",
    "# --- 4. Identify Optimal Lambda ---\n",
    "min_mse = min(test_mses)\n",
    "min_idx = test_mses.index(min_mse)\n",
    "best_lam = lambda_values[min_idx].item()\n",
    "\n",
    "print(f\"\\nRESULTS:\")\n",
    "print(f\"Optimal λ: {best_lam:.4f} (Log: {np.log10(best_lam):.2f})\")\n",
    "print(f\"Min Test MSE: {min_mse:.4f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Format data for Block 4\n",
    "lambda_log = torch.log10(lambda_values)\n",
    "train_mses = torch.tensor(train_mses)\n",
    "test_mses = torch.tensor(test_mses)\n",
    "beta_norms = torch.tensor(beta_norms)\n",
    "condition_numbers = torch.tensor(condition_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE BLOCK 3A: Sweeping the Hyperparameter λ\n",
    "\n",
    "# --- 1. Define Lambda Range (logarithmic sweep) ---\n",
    "# A broad range is needed to demonstrate the full effect\n",
    "lambda_min, lambda_max = -8, 10  # 10^-8 to 10^4\n",
    "lambda_values = torch.logspace(lambda_min, lambda_max, 50) \n",
    "\n",
    "# Use a power basis matrix for the overfitted problem (Degree 12)\n",
    "DEGREE_ILL = 12 \n",
    "X = construct_vandermonde_matrix(x_train, DEGREE_ILL)\n",
    "X_test = construct_vandermonde_matrix(x_test, DEGREE_ILL)\n",
    "\n",
    "# --- 2. Initialize storage for results ---\n",
    "train_mses = []\n",
    "test_mses = []\n",
    "beta_norms = []\n",
    "condition_numbers = []\n",
    "\n",
    "# --- 3. Iterate and Solve Analytically ---\n",
    "print(\"=\" * 60)\n",
    "print(f\"Analyzing Ridge Regression (Degree {DEGREE_ILL}) over 50 λ values\")\n",
    "print(\"-\" * 60)\n",
    "for lambda_val in lambda_values:\n",
    "    # A. Solve the analytical Ridge equation\n",
    "    beta_ridge, cond_num = solve_ridge_analytical(X, y_train, lambda_val.item())\n",
    "    \n",
    "    # B. Predict on train and test sets\n",
    "    y_train_pred = torch.einsum('ij,jk->ik', X, beta_ridge)\n",
    "    y_test_pred = torch.einsum('ij,jk->ik', X_test, beta_ridge)\n",
    "    \n",
    "    # C. Compute and store metrics\n",
    "    train_mses.append(compute_mse(y_train, y_train_pred))\n",
    "    test_mses.append(compute_mse(y_test, y_test_pred))\n",
    "    \n",
    "    # The L2 norm of the full vector is the penalty term\n",
    "    beta_norms.append(torch.norm(beta_ridge).item())\n",
    "    condition_numbers.append(cond_num)\n",
    "\n",
    "print(\"Analysis complete.\")\n",
    "print(f\"Min Test MSE: {min(test_mses):.4f} (Avg OLS Test MSE was: {test_mses[0]:.4f})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store results as tensors for plotting ease\n",
    "lambda_log = torch.log10(lambda_values)\n",
    "train_mses = torch.tensor(train_mses)\n",
    "test_mses = torch.tensor(test_mses)\n",
    "beta_norms = torch.tensor(beta_norms)\n",
    "condition_numbers = torch.tensor(condition_numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Sweep \n",
    "\n",
    "This is essential for setting up the double descent narrative. We sweep polynomial degree from 1 to ~30, approaching the interpolation threshold (where p ≈ n). For each degree, we compare:\n",
    "\n",
    "1. **Unregularized OLS** (using pseudoinverse for all degrees, as established in Part 2)\n",
    "2. **Optimal Ridge** (λ chosen to minimize test MSE)\n",
    "3. **Heavy Ridge** (large λ, high bias but stable)\n",
    "\n",
    "This reveals the classical U-shape (where regularization helps) and the transition point where conventional analysis breaks down, setting up Part 5's double descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree Sweep \n",
    "\n",
    "# --- Sweep polynomial degree from 1 to ~30 (approaching interpolation threshold) ---\n",
    "# The interpolation threshold occurs when p (parameters) ≈ n (data points)\n",
    "# For our data: n_train = 160, so threshold is around degree 159\n",
    "# We'll go up to degree 30 to show the transition region clearly\n",
    "\n",
    "degrees_range = list(range(1, 320))  # Degrees 1 to 30\n",
    "n_train_samples = x_train.shape[0]\n",
    "\n",
    "# Storage for results\n",
    "test_mse_ols = []\n",
    "test_mse_optimal_ridge = []\n",
    "test_mse_heavy_ridge = []\n",
    "beta_norms_ols = []\n",
    "beta_norms_optimal_ridge = []\n",
    "beta_norms_heavy_ridge = []\n",
    "condition_numbers_ols = []\n",
    "condition_numbers_ridge = []\n",
    "\n",
    "# For optimal Ridge, we'll do a quick lambda search for each degree\n",
    "# (using a smaller grid for efficiency)\n",
    "lambda_candidates = torch.logspace(-4, 2, 20)  # Reasonable range for most degrees\n",
    "heavy_lambda = 10.0  # Fixed large lambda for heavy regularization\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Degree Sweep: Comparing OLS, Optimal Ridge, and Heavy Ridge\")\n",
    "print(f\"Training samples: {n_train_samples} (interpolation threshold: degree ≈ {n_train_samples-1})\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for degree in degrees_range:\n",
    "    # Construct design matrices\n",
    "    X_train = construct_vandermonde_matrix(x_train, degree)\n",
    "    X_test = construct_vandermonde_matrix(x_test, degree)\n",
    "    n_params = X_train.shape[1]\n",
    "    \n",
    "    # --- 1. Unregularized OLS using pseudoinverse (as established in Part 2) ---\n",
    "    # This is mathematically honest and prevents runtime errors\n",
    "    # Near the interpolation threshold (p ≈ n), X^T X becomes singular/ill-conditioned\n",
    "    # Pseudoinverse handles these cases gracefully\n",
    "    beta_ols = solve_pseudoinverse(X_train, y_train)\n",
    "    y_test_pred_ols = torch.einsum('ij,jk->ik', X_test, beta_ols)\n",
    "    test_mse_ols.append(compute_mse(y_test, y_test_pred_ols))\n",
    "    beta_norms_ols.append(torch.norm(beta_ols).item())\n",
    "    \n",
    "    # Compute condition number for OLS (may be very large)\n",
    "    try:\n",
    "        XTX_ols = torch.einsum('ni,nj->ij', X_train, X_train)\n",
    "        cond_ols = torch.linalg.cond(XTX_ols).item()\n",
    "        condition_numbers_ols.append(cond_ols)\n",
    "    except:\n",
    "        condition_numbers_ols.append(float('inf'))\n",
    "    \n",
    "    # --- 2. Optimal Ridge (find λ that minimizes test MSE) ---\n",
    "    best_test_mse = float('inf')\n",
    "    best_lambda = None\n",
    "    best_beta = None\n",
    "    \n",
    "    for lambda_candidate in lambda_candidates:\n",
    "        beta_ridge, _ = solve_ridge_analytical(X_train, y_train, lambda_candidate.item())\n",
    "        y_test_pred_ridge = torch.einsum('ij,jk->ik', X_test, beta_ridge)\n",
    "        test_mse_ridge = compute_mse(y_test, y_test_pred_ridge)\n",
    "        \n",
    "        if test_mse_ridge < best_test_mse:\n",
    "            best_test_mse = test_mse_ridge\n",
    "            best_lambda = lambda_candidate.item()\n",
    "            best_beta = beta_ridge\n",
    "    \n",
    "    test_mse_optimal_ridge.append(best_test_mse)\n",
    "    beta_norms_optimal_ridge.append(torch.norm(best_beta).item())\n",
    "    \n",
    "    # --- 3. Heavy Ridge (fixed large λ) ---\n",
    "    beta_heavy, cond_ridge = solve_ridge_analytical(X_train, y_train, heavy_lambda)\n",
    "    y_test_pred_heavy = torch.einsum('ij,jk->ik', X_test, beta_heavy)\n",
    "    test_mse_heavy_ridge.append(compute_mse(y_test, y_test_pred_heavy))\n",
    "    beta_norms_heavy_ridge.append(torch.norm(beta_heavy).item())\n",
    "    condition_numbers_ridge.append(cond_ridge)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if degree % 5 == 0:\n",
    "        print(f\"Degree {degree:2d}: OLS MSE={test_mse_ols[-1]:.4f}, \"\n",
    "              f\"Opt Ridge (λ={best_lambda:.2e}) MSE={best_test_mse:.4f}, \"\n",
    "              f\"Heavy Ridge MSE={test_mse_heavy_ridge[-1]:.4f}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Degree sweep complete.\")\n",
    "print(f\"Interpolation threshold: p = n at degree ≈ {n_train_samples - 1}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Convert to tensors for plotting\n",
    "test_mse_ols = torch.tensor(test_mse_ols)\n",
    "test_mse_optimal_ridge = torch.tensor(test_mse_optimal_ridge)\n",
    "test_mse_heavy_ridge = torch.tensor(test_mse_heavy_ridge)\n",
    "degrees_tensor = torch.tensor(degrees_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "### Theory: Interpreting the Regularization Landscape\n",
    "\n",
    "This visualization synthesizes all concepts and sets up the double descent narrative:\n",
    "\n",
    "1. **Classical Regime (p < n)**: The U-shaped curve where regularization prevents overfitting\n",
    "2. **Transition (p ≈ n)**: The interpolation threshold where OLS becomes unstable\n",
    "3. **Overparameterized Regime (p > n)**: Preview of where double descent occurs (Part 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Primary Visualization \n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "fig.suptitle('Regularization Analysis: The Complete Picture', fontsize=16, weight='bold')\n",
    "\n",
    "# Plot 1: PRIMARY - Test MSE vs Polynomial Degree (Three Curves)\n",
    "axes[0].plot(degrees_tensor.numpy(), test_mse_ols.numpy(), 'r-o', \n",
    "             linewidth=2, markersize=5, label='Unregularized OLS', alpha=0.8)\n",
    "axes[0].plot(degrees_tensor.numpy(), test_mse_optimal_ridge.numpy(), 'b-s', \n",
    "             linewidth=2, markersize=5, label='Optimal Ridge', alpha=0.8)\n",
    "axes[0].plot(degrees_tensor.numpy(), test_mse_heavy_ridge.numpy(), 'k-^', \n",
    "             linewidth=2, markersize=5, label='Heavy Ridge (λ=10)', alpha=0.7)\n",
    "\n",
    "# Annotate interpolation threshold (p = n)\n",
    "interpolation_threshold = n_train_samples - 1\n",
    "axes[0].axvline(interpolation_threshold, color='gray', linestyle='--', \n",
    "                linewidth=1.5, alpha=0.5, label=f'Interpolation Threshold (p=n)')\n",
    "\n",
    "axes[0].set_xlabel('Polynomial Degree (Model Complexity)', fontsize=12)\n",
    "axes[0].set_ylabel('Test MSE', fontsize=12)\n",
    "axes[0].set_title('Test MSE vs. Model Complexity\\n(Classical U-Shape and Transition Region)', fontsize=13)\n",
    "axes[0].legend(fontsize=10, loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add text annotations\n",
    "axes[0].text(0.05, 0.95, 'Classical Regime\\n(p < n)\\nRegularization helps', \n",
    "             transform=axes[0].transAxes, fontsize=10, \n",
    "             verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', \n",
    "             facecolor='lightblue', alpha=0.5))\n",
    "axes[0].text(0.65, 0.95, 'Transition Region\\n(p ≈ n)\\nConventional analysis\\nbreaks down', \n",
    "             transform=axes[0].transAxes, fontsize=10, \n",
    "             verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', \n",
    "             facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Plot 2: Lambda Sweep Results (for degree 12)\n",
    "min_test_mse_idx = torch.argmin(test_mses)\n",
    "lambda_optimal = lambda_values[min_test_mse_idx]\n",
    "\n",
    "axes[1].plot(lambda_log.numpy(), train_mses.numpy(), 'o-', \n",
    "             linewidth=2, markersize=4, label='Training MSE', color='blue', alpha=0.7)\n",
    "axes[1].plot(lambda_log.numpy(), test_mses.numpy(), 's-', \n",
    "             linewidth=2, markersize=4, label='Test MSE', color='red', alpha=0.8)\n",
    "axes[1].axvline(lambda_log[min_test_mse_idx].item(), color='k', \n",
    "               linestyle='--', linewidth=1, label='Optimal $\\\\lambda$')\n",
    "\n",
    "axes[1].set_xlabel('$\\\\log_{10}(\\\\lambda)$', fontsize=12)\n",
    "axes[1].set_ylabel('Mean Squared Error', fontsize=12)\n",
    "axes[1].set_title(f'Generalization Error vs. Regularization Strength $\\\\lambda$\\n(Degree {DEGREE_ILL} Polynomial)', fontsize=13)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0, max(test_mses.max().item(), train_mses.max().item()) * 1.1)\n",
    "\n",
    "axes[1].text(-7, axes[1].get_ylim()[1] * 0.9, 'Overfitting\\n(High Variance)', \n",
    "             fontsize=10, horizontalalignment='left', \n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='salmon', alpha=0.3))\n",
    "axes[1].text(3, axes[1].get_ylim()[1] * 0.9, 'Underfitting\\n(High Bias)', \n",
    "             fontsize=10, horizontalalignment='right', \n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Secondary Visualizations ---\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Secondary Visualizations: Regularization Effects', fontsize=16, weight='bold')\n",
    "\n",
    "# Plot 1: Beta Norm Decay vs log(λ) (from lambda sweep)\n",
    "axes[0, 0].plot(lambda_log.numpy(), beta_norms.numpy(), 'k-', \n",
    "                linewidth=2, markersize=4, label='||$\\\\beta||_{2}$ Norm', alpha=0.8)\n",
    "axes[0, 0].axvline(lambda_log[min_test_mse_idx].item(), color='k', \n",
    "                   linestyle='--', linewidth=1)\n",
    "axes[0, 0].set_xlabel('$\\\\log_{10}(\\\\lambda)$', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Coefficient Norm $||\\\\beta||_{2}$', fontsize=12)\n",
    "axes[0, 0].set_title('Model Complexity $||\\\\beta||_{2}$ vs. Regularization Strength $\\\\lambda$', fontsize=13)\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_yscale('log')\n",
    "axes[0, 0].text(lambda_log[min_test_mse_idx].item(), \n",
    "                beta_norms[min_test_mse_idx].item() * 1.5, \n",
    "                f'Optimal $\\\\|\\\\beta\\\\|={beta_norms[min_test_mse_idx].item():.2f}$', \n",
    "                fontsize=10, bbox=dict(boxstyle='round,pad=0.5', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Plot 2: Condition Number Improvement vs log(λ)\n",
    "axes[0, 1].semilogy(lambda_log.numpy(), condition_numbers.numpy(), 'g-', \n",
    "                    linewidth=2, markersize=4, label='Condition Number', alpha=0.8)\n",
    "axes[0, 1].axvline(lambda_log[min_test_mse_idx].item(), color='k', \n",
    "                   linestyle='--', linewidth=1)\n",
    "axes[0, 1].set_xlabel('$\\\\log_{10}(\\\\lambda)$', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Condition Number $\\\\kappa(X^T X + \\\\lambda I)$', fontsize=12)\n",
    "axes[0, 1].set_title('Numerical Stability: Condition Number vs. $\\\\lambda$', fontsize=13)\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].text(-7, condition_numbers[0].item() * 0.5, \n",
    "                'Ill-conditioned\\n(near OLS)', fontsize=10, \n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='salmon', alpha=0.3))\n",
    "axes[0, 1].text(3, condition_numbers[-1].item() * 2, \n",
    "                'Well-conditioned\\n(strong regularization)', fontsize=10, \n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "# Plot 3: Model Fits Comparison (OLS, Optimal Ridge, Heavy Ridge) for degree 12\n",
    "# Solve for OLS (using pseudoinverse), Optimal Ridge, and Heavy Ridge\n",
    "X_plot = construct_vandermonde_matrix(x_train, DEGREE_ILL)\n",
    "beta_ols_plot = solve_pseudoinverse(X_plot, y_train)\n",
    "beta_optimal_plot, _ = solve_ridge_analytical(X_plot, y_train, lambda_optimal.item())\n",
    "beta_heavy_plot, _ = solve_ridge_analytical(X_plot, y_train, heavy_lambda)\n",
    "\n",
    "# Generate smooth x for plotting\n",
    "x_plot_smooth = torch.linspace(x_min, x_max, 300).unsqueeze(1)\n",
    "y_true_plot = 0.5 * x_plot_smooth ** 2\n",
    "\n",
    "y_ols_plot = predict_polynomial(x_plot_smooth, beta_ols_plot)\n",
    "y_optimal_plot = predict_polynomial(x_plot_smooth, beta_optimal_plot)\n",
    "y_heavy_plot = predict_polynomial(x_plot_smooth, beta_heavy_plot)\n",
    "\n",
    "axes[1, 0].scatter(x_train.numpy(), y_train.numpy(), alpha=0.6, \n",
    "                   label='Training data', s=40, color='gray')\n",
    "axes[1, 0].plot(x_plot_smooth.numpy(), y_true_plot.numpy(), 'g--', \n",
    "                linewidth=2, label='True function', alpha=0.7)\n",
    "axes[1, 0].plot(x_plot_smooth.numpy(), y_ols_plot.numpy(), 'r-', \n",
    "                linewidth=2, label=f'OLS (λ≈0) - Overfit', alpha=0.7)\n",
    "axes[1, 0].plot(x_plot_smooth.numpy(), y_optimal_plot.numpy(), 'b-', \n",
    "                linewidth=3, label=f'Ridge (Optimal $\\\\lambda$) - Balanced', alpha=0.9)\n",
    "axes[1, 0].plot(x_plot_smooth.numpy(), y_heavy_plot.numpy(), 'k-', \n",
    "                linewidth=2, label=f'Ridge (Heavy $\\\\lambda$) - Underfit', alpha=0.6)\n",
    "axes[1, 0].set_xlabel('x', fontsize=12)\n",
    "axes[1, 0].set_ylabel('y', fontsize=12)\n",
    "axes[1, 0].set_title(f'Visualizing the Bias-Variance Tradeoff with $\\\\lambda$ (Degree {DEGREE_ILL})', fontsize=13)\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Beta Norm vs Degree (showing complexity control)\n",
    "axes[1, 1].plot(degrees_tensor.numpy(), beta_norms_ols, 'r-o', \n",
    "                linewidth=2, markersize=4, label='OLS $||\\\\beta||_2$', alpha=0.8)\n",
    "axes[1, 1].plot(degrees_tensor.numpy(), beta_norms_optimal_ridge, 'b-s', \n",
    "                linewidth=2, markersize=4, label='Optimal Ridge $||\\\\beta||_2$', alpha=0.8)\n",
    "axes[1, 1].plot(degrees_tensor.numpy(), beta_norms_heavy_ridge, 'k-^', \n",
    "                linewidth=2, markersize=4, label='Heavy Ridge $||\\\\beta||_2$', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Polynomial Degree', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Coefficient Norm $||\\\\beta||_2$', fontsize=12)\n",
    "axes[1, 1].set_title('Model Complexity Control: $||\\\\beta||_2$ vs. Degree', fontsize=13)\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesis Points\n",
    "\n",
    "1. **Regularization as Numerical Stabilization**: The $\\lambda I$ term transforms an ill-conditioned problem (Part 2) into a well-conditioned, strictly convex optimization problem. The condition number reduction demonstrates how Ridge restores numerical stability that the Normal Equation $(X^T X)^{-1}$ lacked.\n",
    "\n",
    "2. **Regularization as Complexity Control**: The penalty term constrains model complexity, preventing overfitting in the classical regime (p < n). The U-shaped test error curve shows the bias-variance tradeoff in action, with optimal $\\lambda$ balancing fit and generalization.\n",
    "\n",
    "3. **The Interpolation Threshold**: As p approaches n, unregularized OLS becomes unstable (as seen in the exploding test MSE), but Ridge remains stable. This transition point is where the classical bias-variance analysis begins to break down, foreshadowing the double descent phenomenon (Part 5).\n",
    "\n",
    "4. **Connection to Part 2**: Regularization provides an analytical solution to the numerical instability that made the Normal Equation fail. The pseudoinverse (Part 2) was a workaround; Ridge is a principled solution that modifies the optimization problem itself.\n",
    "\n",
    "5. **Foreshadowing Part 4**: Iterative optimizers (Part 4) will solve the same regularized problem using gradient descent, Newton's method, and L-BFGS. The implicit bias of these algorithms relates to regularization's explicit bias, providing complementary perspectives on complexity control.\n",
    "\n",
    "6. **Foreshadowing Part 5**: The primary visualization (Test MSE vs Degree) shows the classical U-shape where regularization helps. Beyond the interpolation threshold (p > n), we will observe in Part 5 that test error can decrease again—the double descent phenomenon—challenging conventional wisdom about the bias-variance tradeoff.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# --- 1. CONFIGURATION & HELPERS ---\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def get_design_matrix(x, degree):\n",
    "    \"\"\"Creates Vandermonde matrix (Polynomial features).\"\"\"\n",
    "    return torch.stack([x.squeeze() ** i for i in range(degree + 1)], dim=1)\n",
    "\n",
    "def compute_mse(y_true, y_pred):\n",
    "    \"\"\"Calculates Mean Squared Error.\"\"\"\n",
    "    return torch.mean((y_true - y_pred) ** 2).item()\n",
    "\n",
    "# --- 2. CORE MATH IMPLEMENTATION (The \"White Box\") ---\n",
    "\n",
    "def solve_ridge_analytical(X, y, lambda_, use_cholesky=True):\n",
    "    \"\"\"\n",
    "    Solves Ridge Regression: β = (XᵀX + λI)⁻¹ Xᵀy\n",
    "    Uses Cholesky decomposition for numerical stability on SPD matrices.\n",
    "    \"\"\"\n",
    "    # Ensure 2D shapes (N, 1)\n",
    "    if y.ndim == 1: y = y.view(-1, 1)\n",
    "    n, d = X.shape\n",
    "\n",
    "    # Construct Linear System: Aβ = b\n",
    "    XTX = X.T @ X\n",
    "    XTy = X.T @ y\n",
    "    I = torch.eye(d, device=X.device)\n",
    "    A = XTX + (lambda_ * I)\n",
    "\n",
    "    # Solve\n",
    "    if use_cholesky:\n",
    "        try:\n",
    "            L = torch.linalg.cholesky(A)\n",
    "            beta = torch.cholesky_solve(XTy, L)\n",
    "            return beta\n",
    "        except RuntimeError:\n",
    "            # Fallback for extremely small lambda or numerical noise\n",
    "            pass\n",
    "            \n",
    "    return torch.linalg.solve(A, XTy)\n",
    "\n",
    "# --- 3. EXPERIMENT RUNNER (The \"Sweep\") ---\n",
    "\n",
    "def run_regularization_sweep(x_train, y_train, x_test, y_test, degree, lambda_values):\n",
    "    \"\"\"\n",
    "    Sweeps over lambda values to demonstrate Bias-Variance Tradeoff.\n",
    "    \"\"\"\n",
    "    # A. Feature Construction (Raw/Unscaled to show instability)\n",
    "    X_train = get_design_matrix(x_train, degree)\n",
    "    X_test = get_design_matrix(x_test, degree)\n",
    "    \n",
    "    # Storage\n",
    "    history = {\n",
    "        'lambda': [], \n",
    "        'train_mse': [], \n",
    "        'test_mse': [], \n",
    "        'beta_norm': [], \n",
    "        'condition_number': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Running Sweep: Degree={degree}, Lambdas={len(lambda_values)}...\")\n",
    "    \n",
    "    for lam in lambda_values:\n",
    "        lambda_val = lam.item()\n",
    "        \n",
    "        # 1. Solve\n",
    "        beta = solve_ridge_analytical(X_train, y_train, lambda_val)\n",
    "        \n",
    "        # 2. Predict\n",
    "        y_train_pred = X_train @ beta\n",
    "        y_test_pred = X_test @ beta\n",
    "        \n",
    "        # 3. Record Metrics\n",
    "        # Condition number of the Hessian (X^T X + λI)\n",
    "        H = X_train.T @ X_train + lambda_val * torch.eye(X_train.shape[1])\n",
    "        cond_no = torch.linalg.cond(H).item()\n",
    "        \n",
    "        history['lambda'].append(lambda_val)\n",
    "        history['train_mse'].append(compute_mse(y_train, y_train_pred))\n",
    "        history['test_mse'].append(compute_mse(y_test, y_test_pred))\n",
    "        history['beta_norm'].append(torch.norm(beta).item())\n",
    "        history['condition_number'].append(cond_no)\n",
    "        \n",
    "    return history\n",
    "\n",
    "# --- 4. EXECUTION ---\n",
    "\n",
    "# Setup\n",
    "DEGREE_ILL = 12\n",
    "lambdas = torch.logspace(-2, 10, 50)\n",
    "\n",
    "# Run Experiment\n",
    "results = run_regularization_sweep(x_train, y_train, x_test, y_test, DEGREE_ILL, lambdas)\n",
    "\n",
    "# Find Optimal\n",
    "best_idx = np.argmin(results['test_mse'])\n",
    "best_lambda = results['lambda'][best_idx]\n",
    "min_mse = results['test_mse'][best_idx]\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Optimal Lambda: {best_lambda:.2e}\")\n",
    "print(f\"Min Test MSE:   {min_mse:.4f}\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
