{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b03c1fc",
   "metadata": {},
   "source": [
    "# Part 5: The Generalized Bias-Variance Tradeoff & Double Descent\n",
    "**A Non-Linear Programming Capstone Project**\n",
    "\n",
    "## 1. Notebook Objective & Theoretical Framework\n",
    "This final notebook synthesizes the analytical methods (Part 2), regularization theory (Part 3), and optimization behavior (Part 4) to demonstrate the **Double Descent** phenomenon.\n",
    "\n",
    "We will move beyond the \"single dataset\" view and adopt the statistical learning perspective (ISL Ch. 2 and Ch. 10.8). By simulating thousands of parallel universes (datasets), we will empirically decompose the Mean Squared Error into **Bias² + Variance + Irreducible Error**.\n",
    "\n",
    "**Core Hypothesis (based on *Schaeffer et al.* & *ISL*):**\n",
    "The \"descent\" in the over-parameterized regime ($p > n$) occurs because, among the infinite solutions that satisfy $X\\beta = y$, the \"natural\" solver (the Moore-Penrose Pseudoinverse or Gradient Descent initialized at zero) selects the solution with the **minimum $\\ell_2$ norm**. This acts as an *implicit* regularization, suppressing the variance that explodes at the interpolation threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2111edc2",
   "metadata": {},
   "source": [
    "### Block 1: The Experimental Design (The Ensemble Generator)\n",
    "**Goal:** Define the infrastructure to calculate \"True\" Bias and Variance.\n",
    "\n",
    "* **Concept:** To measure bias and variance, we cannot use a single training set. We must approximate the expectation over the data distribution $\\mathbb{E}_{\\mathcal{D}}$.\n",
    "* **Implementation Details:**\n",
    "    * Define a `true_function(x)`: $f(x) = \\sin(2\\pi x)$ or the previous $0.5x^2$.\n",
    "    * Create a factory function that generates $K$ distinct datasets (e.g., $K=100$), each with $N$ sample points (e.g., $N=15$).\n",
    "    * **PyTorch/Einsum:** Use broadcasting to generate all $K$ datasets in a single tensor operation for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "342b57ab-b9e9-4df7-a5b1-5158cb1c49d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- numerical stability settings ----\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.set_default_dtype(torch.float64)   # <-- use double precision globally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a1c8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1000 datasets with 15 samples each.\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "K_datasets = 1000   # Number of parallel universes (datasets)\n",
    "N_samples = 15      # Number of training points per dataset (Interpolation threshold at d=14)\n",
    "sigma_noise = 0.2   # Irreducible error (noise level)\n",
    "test_size = 1000    # Size of the test set for integral approximation\n",
    "\n",
    "# 1. Define the True Function\n",
    "def true_function(x):\n",
    "    # Using sin(2*pi*x) as it's a classic choice for showing wiggles, \n",
    "    # but we can also use 0.5*x**2 if preferred for continuity with Part 1.\n",
    "    # Let's use a slightly more complex function to justify high degrees.\n",
    "    return torch.sin(2 * torch.pi * x)\n",
    "\n",
    "# 2. The Ensemble Generator (Factory)\n",
    "def generate_ensemble(K, N, sigma):\n",
    "    \"\"\"\n",
    "    Generates K datasets, each with N points.\n",
    "    Returns:\n",
    "        X: (K, N) tensor of inputs\n",
    "        y: (K, N) tensor of targets\n",
    "    \"\"\"\n",
    "    # Random x in [-1, 1]\n",
    "    X = torch.rand(K, N) * 2 - 1\n",
    "    \n",
    "    # y = f(x) + epsilon\n",
    "    noise = torch.randn(K, N) * sigma\n",
    "    y = true_function(X) + noise\n",
    "    return X, y\n",
    "\n",
    "# Generate the Test Set (Fixed for all models)\n",
    "X_test = torch.linspace(-1, 1, test_size).view(-1, 1) # (T, 1)\n",
    "y_test_true = true_function(X_test)                   # (T, 1)\n",
    "\n",
    "print(f\"Generated {K_datasets} datasets with {N_samples} samples each.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aba6bb7-c789-4bb7-86fc-2a508aa913e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Block 1: The Experimental Design (Ensemble Generator) ---\n",
    "# # Configuration\n",
    "# K_datasets = 1000   # number of independent datasets (\"parallel universes\")\n",
    "# N_samples  = 15     # samples per dataset  -> interpolation threshold at degree d = N_samples - 1 = 14\n",
    "# sigma_noise = 0.2   # irreducible noise std\n",
    "# test_size   = 1000  # dense test grid for expectation over x\n",
    "\n",
    "# # 1) True function: same as Part 1\n",
    "# def true_function(x):\n",
    "#     return 0.5 * x**2\n",
    "\n",
    "# # 2) Ensemble generator: vectorized over K\n",
    "# def generate_ensemble(K, N, sigma):\n",
    "#     \"\"\"\n",
    "#     Returns:\n",
    "#         X: (K, N) tensor of inputs in [-1, 1]\n",
    "#         y: (K, N) tensor of noisy targets: y = f(x) + ε, ε ~ N(0, sigma^2)\n",
    "#     \"\"\"\n",
    "#     X = torch.rand(K, N) * 2 - 1                 # uniform in [-1, 1]\n",
    "#     noise = torch.randn(K, N) * sigma\n",
    "#     y = true_function(X) + noise\n",
    "#     return X, y\n",
    "\n",
    "# # 3) Fixed test grid shared by all models\n",
    "# X_test = torch.linspace(-1, 1, test_size).view(-1, 1)  # (T, 1)\n",
    "# y_test_true = true_function(X_test)                    # (T, 1)\n",
    "\n",
    "# print(f\"Generated {K_datasets} datasets with {N_samples} samples each using f(x)=0.5*x^2.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539fd1f4",
   "metadata": {},
   "source": [
    "### Block 2: The Solver & The Minimum Norm Solution\n",
    "**Goal:** Define the fitting mechanism that operates across both regimes.\n",
    "\n",
    "* **Concept:** We need a solver that works for $p < n$ (Classical) and $p > n$ (Over-parameterized).\n",
    "* **Mathematical Rigor:**\n",
    "    * For $p \\le n$ (Under-parameterized): The solution is unique (if $X$ is full rank). $\\hat{\\beta} = (X^T X)^{-1} X^T y$.\n",
    "    * For $p > n$ (Over-parameterized): The system is underdetermined. There are infinite solutions. We explicitly choose the **Minimum Norm Solution**: $\\hat{\\beta} = X^T (X X^T)^{-1} y$ (using the pseudoinverse definition).\n",
    "* **Implementation:** A function taking degree $d$ and the dataset, constructing the Vandermonde matrix via `einsum`, and solving via `torch.linalg.pinv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c394eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit_ensemble(X_train_ensemble, y_train_ensemble, degree):\n",
    "#     \"\"\"\n",
    "#     Fits polynomial models of 'degree' to all K datasets simultaneously.\n",
    "    \n",
    "#     Args:\n",
    "#         X_train_ensemble: (K, N) tensor\n",
    "#         y_train_ensemble: (K, N) tensor\n",
    "#         degree: int\n",
    "        \n",
    "#     Returns:\n",
    "#         betas: (K, d+1) tensor of coefficients\n",
    "#     \"\"\"\n",
    "#     K, N = X_train_ensemble.shape\n",
    "    \n",
    "#     # 1. Construct Vandermonde Matrix for all K datasets\n",
    "#     # We want a tensor of shape (K, N, d+1)\n",
    "#     # Powers: [0, 1, ..., d]\n",
    "#     powers = torch.arange(degree + 1).float()\n",
    "    \n",
    "#     # Broadcasting magic:\n",
    "#     # X_train_ensemble.unsqueeze(-1) is (K, N, 1)\n",
    "#     # powers is (d+1)\n",
    "#     # Result is (K, N, d+1)\n",
    "#     Phi = X_train_ensemble.unsqueeze(-1) ** powers\n",
    "    \n",
    "#     # 2. Solve for Beta using Pseudoinverse (Minimum Norm Solution)\n",
    "#     # torch.linalg.pinv handles the batch dimension K automatically\n",
    "#     # Phi: (K, N, d+1)\n",
    "#     # y: (K, N)\n",
    "    \n",
    "#     # We need y to be (K, N, 1) for matrix multiplication compatibility if we were doing it manually,\n",
    "#     # but pinv expects standard shapes. Let's check pinv docs or usage.\n",
    "#     # pinv(A) @ B is the typical solve.\n",
    "    \n",
    "#     # Compute pseudoinverse of Phi: (K, d+1, N)\n",
    "#     Phi_pinv = torch.linalg.pinv(Phi)\n",
    "    \n",
    "#     # Beta = Phi_pinv @ y\n",
    "#     # (K, d+1, N) @ (K, N, 1) -> (K, d+1, 1)\n",
    "#     betas = torch.matmul(Phi_pinv, y_train_ensemble.unsqueeze(-1))\n",
    "    \n",
    "#     return betas.squeeze(-1) # (K, d+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "653e2bde-bdee-4ec2-9481-0818160d951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit_ensemble(X_train_ensemble, y_train_ensemble, degree, rcond=1e-6):\n",
    "  \n",
    "#     K, N = X_train_ensemble.shape\n",
    "#     powers = torch.arange(degree + 1, dtype=torch.get_default_dtype())\n",
    "\n",
    "    \n",
    "#     Phi = X_train_ensemble.unsqueeze(-1) ** powers\n",
    "\n",
    "\n",
    "#     Phi_pinv = torch.linalg.pinv(Phi, rcond=rcond)\n",
    "\n",
    "\n",
    "#     betas = torch.matmul(Phi_pinv, y_train_ensemble.unsqueeze(-1))\n",
    "\n",
    "#     return betas.squeeze(-1)  # (K, d+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ad1ce4-ed46-48bd-9af6-4e049bf85e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 2: The Solver (Legendre Basis + Minimum Norm Solution) ---\n",
    "import torch\n",
    "\n",
    "def fit_ensemble_legendre(X_train_ensemble, y_train_ensemble, degree, rcond=1e-6):\n",
    "    \"\"\"\n",
    "    Fits Legendre polynomial models of given 'degree' to all K datasets simultaneously.\n",
    "    Uses minimum-norm least squares via pseudoinverse.\n",
    "    Arguments:\n",
    "        X_train_ensemble : (K, N) tensor\n",
    "        y_train_ensemble : (K, N) tensor\n",
    "        degree           : polynomial degree\n",
    "        rcond             : small cutoff for numerical stability\n",
    "    Returns:\n",
    "        betas : (K, degree+1) tensor of coefficients for each Legendre basis term\n",
    "    \"\"\"\n",
    "    from torch.special import legendre_polynomial\n",
    "\n",
    "    K, N = X_train_ensemble.shape\n",
    "    # Evaluate Legendre basis for all degrees 0..degree\n",
    "    Phi = torch.stack(\n",
    "        [legendre_polynomial(d)(X_train_ensemble) for d in range(degree + 1)],\n",
    "        dim=-1\n",
    "    )  # shape (K, N, d+1)\n",
    "\n",
    "    # Pseudoinverse with regularization cutoff\n",
    "    Phi_pinv = torch.linalg.pinv(Phi, rcond=rcond)\n",
    "\n",
    "    # Minimum-norm coefficients\n",
    "    betas = torch.matmul(Phi_pinv, y_train_ensemble.unsqueeze(-1))\n",
    "    return betas.squeeze(-1)  # (K, d+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e433ec",
   "metadata": {},
   "source": [
    "### Block 3: The Large-Scale Experiment (The Loop)\n",
    "**Goal:** Collect error metrics across the complexity spectrum.\n",
    "\n",
    "* **Methodology:**\n",
    "    * Iterate through model degrees $d$ from 1 to n.\n",
    "    * For each degree $d$:\n",
    "        1.  Fit models to all $K$ datasets simultaneously.\n",
    "        2.  Evaluate predictions on a large, fixed **Test Set**.\n",
    "        3.  Calculate **Bias²**: $(\\mathbb{E}[\\hat{f}(x)] - f(x))^2$.\n",
    "        4.  Calculate **Variance**: $\\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2]$.\n",
    "        5.  Calculate **MSE**: Bias² + Variance + Noise.\n",
    "    * **Efficiency:** Heavily vectorized operations using Einstein summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7599f5bc-eaf8-4df5-b40a-5953614df40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def legendre_basis(x, degree):\n",
    "    \"\"\"\n",
    "    Build a Legendre polynomial basis [P0(x), P1(x), ..., Pd(x)]\n",
    "    using the recurrence relation:\n",
    "        P0 = 1\n",
    "        P1 = x\n",
    "        Pn = ((2n-1)xPn-1 - (n-1)Pn-2)/n\n",
    "    Args:\n",
    "        x : tensor of shape (K, N) or (N,)\n",
    "        degree : highest polynomial degree\n",
    "    Returns:\n",
    "        Phi : tensor of shape (..., degree+1)\n",
    "    \"\"\"\n",
    "    P = [torch.ones_like(x), x]\n",
    "    for n in range(2, degree + 1):\n",
    "        Pn = ((2*n - 1) * x * P[-1] - (n - 1) * P[-2]) / n\n",
    "        P.append(Pn)\n",
    "    return torch.stack(P[:degree + 1], dim=-1)\n",
    "\n",
    "\n",
    "def fit_ensemble_legendre(X_train_ensemble, y_train_ensemble, degree, rcond=1e-6):\n",
    "    \"\"\"\n",
    "    Fits Legendre polynomial models (degree) to all K datasets simultaneously.\n",
    "    Uses minimum-norm LS with a pseudoinverse cutoff rcond.\n",
    "    \"\"\"\n",
    "    K, N = X_train_ensemble.shape\n",
    "\n",
    "    # Build Legendre Vandermonde: shape (K, N, d+1)\n",
    "    Phi = legendre_basis(X_train_ensemble, degree)\n",
    "\n",
    "    # Compute pseudoinverse with small cutoff for stability\n",
    "    Phi_pinv = torch.linalg.pinv(Phi, rcond=rcond)\n",
    "\n",
    "    # Solve minimum-norm coefficients\n",
    "    betas = torch.matmul(Phi_pinv, y_train_ensemble.unsqueeze(-1))\n",
    "\n",
    "    return betas.squeeze(-1)  # (K, d+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4ce90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Storage for metrics\n",
    "# n=100\n",
    "# degrees = range(1, n)\n",
    "# bias_squared_history = []\n",
    "# variance_history = []\n",
    "# mse_history = []\n",
    "# avg_norm_history = []\n",
    "\n",
    "# # Generate the datasets once\n",
    "# X_train_K, y_train_K = generate_ensemble(K_datasets, N_samples, sigma_noise)\n",
    "\n",
    "# print(\"Starting Double Descent Experiment...\")\n",
    "\n",
    "# for d in degrees:\n",
    "#     # 1. Fit models\n",
    "#     betas = fit_ensemble(X_train_K, y_train_K, d) # (K, d+1)\n",
    "    \n",
    "#     # 2. Store Norm of parameters (for Block 5)\n",
    "#     # Average L2 norm across K models\n",
    "#     avg_norm = torch.mean(torch.norm(betas, p=2, dim=1)).item()\n",
    "#     avg_norm_history.append(avg_norm)\n",
    "    \n",
    "#     # 3. Make Predictions on Test Set\n",
    "#     # Construct Test Vandermonde: (T, d+1)\n",
    "#     powers = torch.arange(d + 1).float()\n",
    "#     Phi_test = X_test ** powers # (T, d+1)\n",
    "    \n",
    "#     # Predictions: (K, T)\n",
    "#     # We want y_pred[k, t] = Phi_test[t] @ betas[k]\n",
    "#     # betas: (K, d+1), Phi_test: (T, d+1)\n",
    "#     # Result: (K, T)\n",
    "#     y_preds = torch.matmul(betas, Phi_test.T) \n",
    "    \n",
    "#     # 4. Calculate Bias and Variance Decomposition\n",
    "    \n",
    "#     # Expected Prediction E[f_hat(x)] over datasets: (T,)\n",
    "#     main_prediction = torch.mean(y_preds, dim=0)\n",
    "    \n",
    "#     # Bias^2: (E[f_hat] - f_true)^2\n",
    "#     # Average over test points\n",
    "#     bias_sq = torch.mean((main_prediction.unsqueeze(-1) - y_test_true) ** 2).item()\n",
    "    \n",
    "#     # Variance: E[(f_hat - E[f_hat])^2]\n",
    "#     # Variance per test point, then averaged\n",
    "#     # variance = torch.mean(torch.var(y_preds, dim=0)).item()\n",
    "#     # Variance: average over test points of the across-dataset variance\n",
    "#     variance = torch.mean(torch.var(y_preds, dim=0, unbiased=False)).item()\n",
    "\n",
    "#     # MSE: Average prediction error on test set\n",
    "#     # Mean over K, Mean over T\n",
    "#     # (y_preds - y_test_true.T)^2\n",
    "#     mse = torch.mean((y_preds - y_test_true.T) ** 2).item()\n",
    "    \n",
    "#     bias_squared_history.append(bias_sq)\n",
    "#     variance_history.append(variance)\n",
    "#     mse_history.append(mse)\n",
    "    \n",
    "#     if d % 5 == 0:\n",
    "#         print(f\"Degree {d}: MSE={mse:.4f}, Bias^2={bias_sq:.4f}, Var={variance:.4f}\")\n",
    "\n",
    "# print(\"Experiment Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adfdb3a",
   "metadata": {},
   "source": [
    "### Block 4: Visualization of the Double Descent\n",
    "**Goal:** The \"Money Plot\" (reproducing the YouTube video and ISL Figure 10.24).\n",
    "\n",
    "* **Visuals:** A single figure with three overlaid curves:\n",
    "    1.  **Bias² (Monotonic Decrease)**\n",
    "    2.  **Variance (The Bell Curve)**\n",
    "    3.  **Test Error (The Double Descent)**\n",
    "* **Annotation:** Explicitly mark the **Interpolation Threshold** ($p=n$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9dd831f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'degrees' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m degrees_np = np.array(\u001b[38;5;28mlist\u001b[39m(\u001b[43mdegrees\u001b[49m))\n\u001b[32m      5\u001b[39m bias_np    = np.array(bias_squared_history)\n\u001b[32m      6\u001b[39m var_np     = np.array(variance_history)\n",
      "\u001b[31mNameError\u001b[39m: name 'degrees' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "degrees_np = np.array(list(degrees))\n",
    "bias_np    = np.array(bias_squared_history)\n",
    "var_np     = np.array(variance_history)\n",
    "mse_np     = np.array(mse_history)\n",
    "\n",
    "# proportions (avoid any divide-by-zero)\n",
    "den = np.maximum(mse_np, 1e-12)\n",
    "bias_prop = bias_np / den\n",
    "var_prop  = var_np  / den\n",
    "\n",
    "threshold_degree = N_samples - 1\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# --- Primary axis: absolute errors (log scale)\n",
    "l1, = ax1.plot(degrees_np, bias_np, label='Bias²', linewidth=2, linestyle='--')\n",
    "l2, = ax1.plot(degrees_np, var_np,  label='Variance', linewidth=2, linestyle='--')\n",
    "l3, = ax1.plot(degrees_np, mse_np,  label='Test MSE (Risk)', linewidth=3)\n",
    "\n",
    "ax1.axhline(y=sigma_noise**2, linestyle=':', label='Irreducible Error (Noise)')\n",
    "ax1.axvline(x=threshold_degree, color='black', linestyle='-', alpha=0.5)\n",
    "ax1.text(threshold_degree + 0.5, np.nanmax(mse_np)*0.8, 'Interpolation Threshold\\n(p = n)', fontsize=10)\n",
    "\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_xlabel('Polynomial Degree (Complexity)', fontsize=12)\n",
    "ax1.set_ylabel('Error (log scale)', fontsize=12)\n",
    "ax1.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "# --- Secondary axis: proportions (linear scale 0..1)\n",
    "ax2 = ax1.twinx()\n",
    "p1, = ax2.plot(degrees_np, bias_prop, label='Bias² / MSE', alpha=0.9)\n",
    "p2, = ax2.plot(degrees_np, var_prop,  label='Var / MSE',  alpha=0.9)\n",
    "ax2.set_ylabel('Proportion of MSE (linear scale)')\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# --- Combined legend\n",
    "handles = [l1, l2, l3, p1, p2]\n",
    "labels  = [h.get_label() for h in handles]\n",
    "ax1.legend(handles, labels, fontsize=11, loc='upper right')\n",
    "\n",
    "plt.title('Double Descent: Errors (log) + Proportions (linear)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea548510",
   "metadata": {},
   "source": [
    "### Block 5: The Norm of the Parameters (Demystifying the Descent)\n",
    "**Goal:** Prove *why* the test error drops in the modern regime.\n",
    "\n",
    "* **Observation:** The norm will spike massively at $p=n$ (fighting to fit noise with limited freedom) and *decrease* as $p$ increases (the \"Minimum Norm\" effect).\n",
    "* **Conclusion:** In the over-parameterized regime, the extra dimensions allow the model to fit the training data perfectly while maintaining a *smaller* total vector norm than at the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd683a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(degrees, avg_norm_history, color='purple', linewidth=2)\n",
    "\n",
    "# Mark Threshold\n",
    "plt.axvline(x=threshold_degree, color='black', linestyle='-', alpha=0.5)\n",
    "plt.text(threshold_degree + 0.5, max(avg_norm_history)*0.8, 'Interpolation Threshold', fontsize=10)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Polynomial Degree', fontsize=12)\n",
    "plt.ylabel('Average L2 Norm of Beta (Log Scale)', fontsize=12)\n",
    "plt.title('Parameter Norm vs. Complexity', fontsize=16)\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a35e7-37e9-4768-8721-1a3da1699cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
